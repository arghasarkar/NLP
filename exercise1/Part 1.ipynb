{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Project\n",
    "\n",
    "Author: Argha Sarkar (1221352) <a.sarkar@warwick.ac.uk>\n",
    "\n",
    "## Part A: Text preprocessing\n",
    "\n",
    "### Declaring file paths for the corpus\n",
    "\n",
    "Over here, the path to the file is declared and it's validated to ensure the file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n"
     ]
    }
   ],
   "source": [
    "from os.path import normpath\n",
    "import os\n",
    "\n",
    "DIRECTORY_PATH = \"signal-news1\"\n",
    "FILE_NAME = \"test_1.jsonl\"\n",
    "FILE_NAME = \"signal-news1.jsonl\"\n",
    "FILE_PATH = normpath(os.path.join(os.getcwd(), DIRECTORY_PATH, FILE_NAME))\n",
    "\n",
    "if os.path.isfile(FILE_PATH):\n",
    "    print(\"The file has been found.\")\n",
    "else:\n",
    "    raise IOError(\"File not found.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the file paths for the positive and negative keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive words file has been found.\n",
      "The negative words file has been found.\n"
     ]
    }
   ],
   "source": [
    "# Subdirectory \n",
    "SUB_DIRECTORY = \"opinion-lexicon-English\"\n",
    "\n",
    "# Read the list of positive words into a hash set \n",
    "POSITIVE_WORDS_FILE_NAME = \"positive-words.txt\"\n",
    "POSITIVE_FILE_PATH = normpath(os.path.join(os.getcwd(), DIRECTORY_PATH, SUB_DIRECTORY, POSITIVE_WORDS_FILE_NAME))\n",
    "\n",
    "if os.path.isfile(POSITIVE_FILE_PATH):\n",
    "    print(\"The positive words file has been found.\")\n",
    "else:\n",
    "    raise IOError(\"File not found: positive words file at path: {}\".format(POSITIVE_FILE_PATH))\n",
    "\n",
    "# Read the list of negative words into a hash set\n",
    "NEGATIVE_WORDS_FILE_NAME = \"negative-words.txt\"\n",
    "NEGATIVE_FILE_PATH = normpath(os.path.join(os.getcwd(), DIRECTORY_PATH, SUB_DIRECTORY, NEGATIVE_WORDS_FILE_NAME))\n",
    "\n",
    "if os.path.isfile(NEGATIVE_FILE_PATH):\n",
    "    print(\"The negative words file has been found.\")\n",
    "else:\n",
    "    raise IOError(\"File not found: negative words file at path: {}\".format(NEGATIVE_FILE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Path\n",
    "\n",
    "The code cell below is used for setting the NLTK path on DCS machines and Joshua. On my personal computer where this assignment was done, I had downloaded the NLTK corpus using **nltk.download()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "if username is not \"ArghaWin10\" or username is not \"arghasarkar\":\n",
    "    import nltk\n",
    "    nltk.data.path.append('/modules/cs918/nltk_data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a corpus object\n",
    "\n",
    "The corpus class loads the converts the *json lines* into separate json documents. It then adds them to a list called **json_docs**. Using a pointer to keep track of the current object, its possible to iterate over the corpus processing each document step by step. This can also be done from any given index and not just from the start. Furthermore, the corpus class also offloads the hassle of dealing with the storage of the preprocessed news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        # Private vars\n",
    "        self._corpus_loaded = False\n",
    "        self._corpus_parsed = False\n",
    "        self._sentence_tokenized = False\n",
    "        \n",
    "        self._document_view_pointer = 0\n",
    "        \n",
    "        self.file_path = file_path \n",
    "        # Checks if the file exists\n",
    "        if os.path.isfile(FILE_PATH):\n",
    "            print(\"The file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found.\")\n",
    "        \n",
    "        self.raw_docs = []\n",
    "        self.json_docs = []\n",
    "        \n",
    "        # Tokenized sentences\n",
    "        self.sent_tokens = []\n",
    "        # Flat list of setences\n",
    "        self.flat_sent_tokens = []\n",
    "        \n",
    "    \n",
    "    def load_corpus(self):\n",
    "        with open(self.file_path, \"r\") as corpus:\n",
    "            for json_line in corpus:\n",
    "                self.raw_docs.append(json_line)\n",
    "        \n",
    "        self._corpus_loaded = True\n",
    "    \n",
    "    \n",
    "    def parse_corpus(self):\n",
    "        for json_line in self.raw_docs:\n",
    "            parsed_json = json.loads(json_line)\n",
    "            self.json_docs.append(parsed_json)\n",
    "        \n",
    "        self._corpus_parsed = True\n",
    "    \n",
    "    \n",
    "    def tokenize_sentences(self):\n",
    "\n",
    "        for article in self.json_docs:\n",
    "            content = article[\"content\"]\n",
    "            sent_tokenized = sent_tokenize(content)\n",
    "            self.sent_tokens.append(sent_tokenized)\n",
    "            \n",
    "        self.flat_sent_tokens = list(itertools.chain.from_iterable(self.sent_tokens))\n",
    "        \n",
    "    \n",
    "    def print_summary_of_corpus(self):\n",
    "        if not self._corpus_loaded:\n",
    "            self.load_corpus()\n",
    "        \n",
    "        if not self._corpus_parsed:\n",
    "            self.parse_corpus()\n",
    "            \n",
    "        if not self._sentence_tokenized:\n",
    "            self.tokenize_sentences()\n",
    "        \n",
    "        print(\"Number of documents: {}.\".format(str(len(self.json_docs))))\n",
    "        print(\"Number of sentences: {}.\".format(str(len(self.flat_sent_tokens))))\n",
    "    \n",
    "    \n",
    "    def print_document(self, doc_idx):\n",
    "        print(\"-- Printing document with index: {} --\".format(doc_idx))\n",
    "        print(self.json_docs[doc_idx])\n",
    "\n",
    "    \n",
    "    def get_document(self, doc_idx):\n",
    "        return self.json_docs[doc_idx]\n",
    "    \n",
    "    \n",
    "    def get_next_document(self):        \n",
    "        self._document_view_pointer += 1\n",
    "        if self._document_view_pointer > len(self.json_docs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.json_docs[(self._document_view_pointer - 1)]\n",
    "        \n",
    "    \n",
    "    def set_current_document(self, json_doc):\n",
    "        self.json_docs[(self._document_view_pointer - 1)] = json_doc\n",
    "    \n",
    "    \n",
    "    def reset_pointer(self):\n",
    "        self._document_view_pointer = 0\n",
    "        \n",
    "    \n",
    "    def set_pointer(self, pointer):\n",
    "        self._document_view_pointer = pointer        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The preprocessing of text takes place here. The following tasks are performed:\n",
    "\n",
    "- All the text is converted to lower case.\n",
    "- All URLs are removed from the text using the following regex: <br>\n",
    "  ```\\b(http)(s)?:\\/\\/((([\\w])(\\.)?)+)\\b```\n",
    "  \n",
    "  The regex looks for boundaries at the start and the end. After that, it looks for \"http\" at the start with an optional \"s\". After that the semi-colon and two slashes are used. After that, any number (one or greater) of words and fullstops will be matched.\n",
    "  \n",
    "  \n",
    "- All numbers are removed which are not concatenated with any non-numeric characters. EG: 5 is removed but not 5pm.\n",
    "- All non-alphanumeric characters expect for spaces are removed.\n",
    "- All words of length below 4 characters are removed. This has been implemented by removing all words of length between one and three characters.\n",
    "  ```\\b[A-Za-z0-9]{1}\\b```\n",
    "  \n",
    "  All boundary alphanumeric characters which are one character in length will be removed.\n",
    "  \n",
    "  \n",
    "Using the **WordNetLemmatizer** from NLTK's stem module, all the words are lemmatized using the default (noun) Part Of Speech tagging. This aims to reduce the vocabulary and make the words more consistent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Coursework:\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        # Debug flag\n",
    "        DEBUG = True\n",
    "        \n",
    "        json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        temp_doc = json_doc\n",
    "        \n",
    "        while json_doc != None:\n",
    "            json_doc[\"content\"] = self.pre_process_document(json_doc)\n",
    "            \n",
    "            # This entire block can be deleted before submission\n",
    "            if DEBUG:\n",
    "                if i < 10:\n",
    "                    with open(\"output.txt\", \"a\") as w:\n",
    "                        w.write(json_doc[\"content\"])\n",
    "                        w.write(\"--------------------------\")\n",
    "\n",
    "                    temp_doc = json_doc\n",
    "                i += 1\n",
    "\n",
    "            # Update the list with the pre-processed document\n",
    "            self.corpus.set_current_document(json_doc)\n",
    "            json_doc = self.corpus.get_next_document()\n",
    "                \n",
    "            \n",
    "    def pre_process_document(self, json_doc):\n",
    "        content = json_doc[\"content\"]\n",
    "        \n",
    "        # Converting it to lower case\n",
    "        content = self.lowercase(content)\n",
    "              \n",
    "        # Remove all URLs\n",
    "        content = self.removeAllURLs(content)\n",
    "        \n",
    "        # Remove all numbers\n",
    "        content = self.removeAllNumbers(content)\n",
    "        \n",
    "        # Removing all non-alphanumeric characters\n",
    "        content = self.removeNonAlphanumericCharacters(content)\n",
    "            \n",
    "        # Removing short words\n",
    "        content = self.removeAllWordsLessThanFourCharsLong(content)\n",
    "        \n",
    "         # Remove white space\n",
    "        content = self.removeNewLines(content)\n",
    "        content = self.removeTabs(content)\n",
    "        content = self.removeMultipleSpaces(content)\n",
    "        \n",
    "        # Using the default lemmatizer\n",
    "        #content = self.lemmatize_document(content)\n",
    "        \n",
    "        return content\n",
    "    \n",
    "\n",
    "    def lemmatize_document(self, content):\n",
    "        lemmatizer = self.lemmatizer\n",
    "        words = content.split()\n",
    "        for i in range(len(words)):\n",
    "            words[i] = lemmatizer.lemmatize(words[i])\n",
    "        return \" \".join(words)\n",
    "\n",
    "    \n",
    "    def lowercase(self, content):\n",
    "        return content.lower()\n",
    "        \n",
    "        \n",
    "    def removeNonAlphanumericCharacters(self, content):\n",
    "        return re.sub(r'[^\\sa-zA-Z0-9]', ' ', content)\n",
    "\n",
    "\n",
    "    def removeNewLines(self, content):\n",
    "        return re.sub(r'\\n',' ', content)\n",
    "    \n",
    "    \n",
    "    def removeMultipleSpaces(self, content):\n",
    "        return re.sub(r'\\s\\s+',' ', content)\n",
    "    \n",
    "    \n",
    "    def removeTabs(self, content):\n",
    "        return re.sub(r'\\t', ' ', content)\n",
    "    \n",
    "    \n",
    "    def removeAllWordsLessThanFourCharsLong(self, content):\n",
    "        #shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "        return re.sub(r'\\b[A-Za-z0-9]{1}\\b', ' ', content)\n",
    "\n",
    "\n",
    "    def removeAllNumbers(self, content):\n",
    "        return re.sub(r'\\b[0-9]+\\b', ' ', content)\n",
    "   \n",
    "\n",
    "    def removeAllURLs(self, content):\n",
    "        return re.sub(r'\\b(http)(s)?:\\/\\/((([\\w])(\\.)?)+)\\b', ' ', content)\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n",
      "Number of documents: 19228.\n",
      "Number of sentences: 270722.\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(FILE_PATH)\n",
    "corpus.print_summary_of_corpus()\n",
    "\n",
    "p = Coursework(corpus)\n",
    "p.process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: N-grams\n",
    "\n",
    "The preprocessing of the data has been performed. \n",
    "\n",
    "For the first part of this task, the vocab size and the number of tokens needs to be calculated. For this, a class has been created which uses a **dictionary** with the words as the key and their counts as the values. The **number of keys** will be the **vocab size** and the **token** will be the **total sum of the word counts**.\n",
    "\n",
    "The **TokenCounter** class uses a dictionary to keep the count of how many times a word has occurred in the corpus so far. If the word is already in the dictionary, it's value is incremented by one. If the word is appearing for the first time, then it's added to the dictionary and it's value is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenCounter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokens_dict = {}\n",
    "    \n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word in self.tokens_dict:\n",
    "            count = self.tokens_dict[word]\n",
    "            count += 1\n",
    "            self.tokens_dict[word] = count\n",
    "        else:\n",
    "            self.tokens_dict[word] = 1\n",
    "\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.tokens_dict.keys())\n",
    "    \n",
    "    \n",
    "    def get_token_count(self):\n",
    "        num_tokens = 0\n",
    "        keys = self.tokens_dict.keys()\n",
    "        for key in keys:\n",
    "            num_tokens += self.tokens_dict[key]\n",
    "        \n",
    "        return num_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class **VocabAndTokens** class iterates through the preprocessed corpus. For each of the preprocessed documents, the text is split into words. After this, each of the word is added to the **TokenCounter** class. After calculating the size of the vocabulary and the number of tokens, it's printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabAndTokens:\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.token_counter = TokenCounter()\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        \n",
    "    def calculate_tokens_and_vocab_size(self):\n",
    "        json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        while json_doc != None:\n",
    "            content = json_doc[\"content\"]\n",
    "            words = content.split()\n",
    "            for word in words:\n",
    "                self.token_counter.add_word(word)\n",
    "            \n",
    "            json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "    \n",
    "    def get_V(self):\n",
    "        return self.token_counter.get_vocab_size()\n",
    "    \n",
    "    \n",
    "    def get_N(self):\n",
    "        return self.token_counter.get_token_count()\n",
    "    \n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.get_V()\n",
    "    \n",
    "    \n",
    "    def get_number_of_tokens(self):\n",
    "        return self.get_N()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B.1) **N** Print out here is the number of tokens and **V** is the vocabulary size.\n",
    "\n",
    "Using the ```get_V()``` and ```get_N()``` methods of the VocabsAndToken counter class, get the vocab size and the total number of tokens. In this case, **V: 102166 N: 5775947.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 102166 N: 5775947\n"
     ]
    }
   ],
   "source": [
    "# Resets it to the first document\n",
    "corpus.reset_pointer()\n",
    "\n",
    "# Initializes the VocabAndTokens class\n",
    "vn = VocabAndTokens(corpus)\n",
    "vn.calculate_tokens_and_vocab_size()\n",
    "\n",
    "print(\"V: {} N: {}\".format(vn.get_V(), vn.get_N()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams calculator: NGramHolder\n",
    "\n",
    "The **NGramHodler** the class  uses a dictionary to store the most common trigrams and provide helper functionalities. For example, it's able to take a list of words as an input and generate a list of n_grams for it. The \"n\" depends on the class instantiation. Furthermore, it's also able to generate the top n_grams in a descending order of count. Given an n_gram, it's able to return how many times it's occured so far in the text processed. Also it's able to search n_grams which begin with a certain phrase. For example, searching for **\"is this\"** using the ```get_n_grams_with_phrase(phrase)``` will return all n_grams which begins with \"is this\" for example: **\"is this the\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "class NGramHolder:\n",
    "    \n",
    "    def __init__(self, n=3):\n",
    "        self.n_grams_dict = {}\n",
    "        self.n = n\n",
    "        \n",
    "    def add_n_grams_from_sentence(self, sentence):\n",
    "        words = sentence.split()\n",
    "        \n",
    "        for i in range(len(words) - (self.n - 1)):\n",
    "            n_gram = \"\"\n",
    "            \n",
    "            for j in range(self.n):\n",
    "                n_gram += words[i + j] + \" \"\n",
    "                \n",
    "            n_gram = n_gram.strip()\n",
    "            self.add_n_gram(n_gram)\n",
    "            \n",
    "    \n",
    "    def add_n_gram(self, n_gram):\n",
    "        if n_gram in self.n_grams_dict:\n",
    "            count = self.n_grams_dict[n_gram]\n",
    "            count += 1\n",
    "            self.n_grams_dict[n_gram] = count\n",
    "        else:\n",
    "            self.n_grams_dict[n_gram] = 1\n",
    "    \n",
    "    \n",
    "    def get_n_gram_count(self, n_gram):\n",
    "        if n_gram in self.n_grams_dict:\n",
    "            return self.n_grams_dict[n_gram]\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def get_top_n_grams(self, top_threshold=25):\n",
    "        sorted_list = sorted(self.n_grams_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        return sorted_list[:top_threshold]\n",
    "\n",
    "    \n",
    "    def get_n_grams_with_phrase(self, phrase):\n",
    "        temp_dict = {}\n",
    "        \n",
    "        for key, value in self.n_grams_dict.items():\n",
    "            if key.find(phrase) == 0:\n",
    "                temp_dict[key] = value\n",
    "                \n",
    "        sorted_list = sorted(temp_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "                \n",
    "        return sorted_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the top 25 trigrams, NGramHolder is used with an initialization value of 3. After that, all the news articles are added using the ```add_n_grams_from_sentence()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriGramCalculator:\n",
    "    \n",
    "    def __init__(self, corpus, n_gram_holder):\n",
    "        self.corpus = corpus\n",
    "        self.n_gram_holder = n_gram_holder\n",
    "\n",
    "        \n",
    "    def calculate_top_trigrams(self, top_threshold=25):\n",
    "        json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        while json_doc != None:\n",
    "            content = json_doc[\"content\"]\n",
    "            self.n_gram_holder.add_n_grams_from_sentence(content)\n",
    "            json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        return self.n_gram_holder.get_top_n_grams(top_threshold)        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B.2) The top 25 trigrams are listed below\n",
    "\n",
    "For the preprecessing that took place earlier, punctuation and all words with fewer than 2 characters were removed. As a result, the trigrams listed below consists of only words which are two or more characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one of the', 2439), ('on shares of', 2093), ('day moving average', 1972), ('on the stock', 1567), ('as well as', 1427), ('in research report', 1417), ('in research note', 1375), ('the year old', 1255), ('the united states', 1225), ('for the quarter', 1221), ('average price of', 1193), ('research report on', 1177), ('research note on', 1138), ('the end of', 1134), ('in report on', 1124), ('earnings per share', 1123), ('shares of the', 1081), ('buy rating to', 1075), ('cell phone plan', 1073), ('phone plan details', 1070), ('according to the', 1068), ('of the company', 1039), ('appeared first on', 995), ('moving average price', 995), ('price target on', 968)]\n"
     ]
    }
   ],
   "source": [
    "# Reset corpus's pointer\n",
    "corpus.reset_pointer()\n",
    "\n",
    "# Number of top trigrams to display\n",
    "TOP_X_NUM_OF_TRIGRAMS = 25\n",
    "\n",
    "trigram_holder = NGramHolder()\n",
    "trigrams_calc = TriGramCalculator(corpus, trigram_holder)\n",
    "\n",
    "top_trigrams = trigrams_calc.calculate_top_trigrams()\n",
    "\n",
    "# Prints of the top 25 trigrams\n",
    "print(top_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B.3) Calculating the list of positive and negative words in the corpus\n",
    "\n",
    "Use the class **WordCount** to read the file with the positive and negative words. After reading the file, the words are stripped of any trailing spaces or new line characters using **rstrip**. I had to do this as I found there were trailing **\\n** characters to these words.\n",
    "\n",
    "After the the words have been added to the dictionary, the **add_word** method can be used for increasing the count for a positive or negative word in the dictionary if it already exists. Alternatively, **add_sentence** can be used for processing all the words in the whole sentence at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCount:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self._words_dict = {}\n",
    "        self._file_path = file_path\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"The file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found: {}.\".format(file_path))\n",
    "            \n",
    "        # Read all the words and add to a hashset\n",
    "        with open(file_path, \"r\") as file:\n",
    "            words_str = file.readlines()\n",
    "        \n",
    "        for word in words_str:\n",
    "            word = word.rstrip()\n",
    "            self._words_dict[word] = 0\n",
    "\n",
    "            \n",
    "    def get_words_counts_dict(self):\n",
    "        return self._words_dict\n",
    "\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word in self._words_dict:\n",
    "            count = self._words_dict[word]\n",
    "            count += 1\n",
    "            self._words_dict[word] = count\n",
    "    \n",
    "    def get_total_word_count(self):\n",
    "        total_count = 0\n",
    "        for word, count in self._words_dict.items():\n",
    "            total_count += count\n",
    "        \n",
    "        return total_count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through all the documents and adds each document to the positive and negative word counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusPositiveNegativeWordsCalculator:\n",
    "    \n",
    "    def __init__(self, pos_file_path, neg_file_path, corpus):\n",
    "        # Positive and negative word counter\n",
    "        self._pos = WordCount(pos_file_path)\n",
    "        self._neg = WordCount(neg_file_path)\n",
    "        \n",
    "        corpus.reset_pointer()\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        self._calculated = False\n",
    "        \n",
    "        \n",
    "    def calculate(self):\n",
    "        json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        while json_doc != None:\n",
    "            content = json_doc[\"content\"]\n",
    "            self._pos.add_sentence(content)\n",
    "            self._neg.add_sentence(content)\n",
    "         \n",
    "            json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        self._calculated = True\n",
    "            \n",
    "\n",
    "    def get_positive_word_count(self):\n",
    "        if self._calculated == False:\n",
    "            self.calculate()\n",
    "        return self._pos.get_total_word_count()\n",
    "    \n",
    "    \n",
    "    def get_negative_word_count(self):\n",
    "        if self._calculated == False:\n",
    "            self.calculate()\n",
    "        return self._neg.get_total_word_count()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the positive and negative words. After that, it prints out the counts of the positive and negative words in the corpus and draws a bar graph to show the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n",
      "The file has been found.\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Resets it to the first document\n",
    "corpus.reset_pointer()\n",
    "corpus_pos_neg_words_calc = CorpusPositiveNegativeWordsCalculator(POSITIVE_FILE_PATH, NEGATIVE_FILE_PATH, corpus)\n",
    "corpus_pos_neg_words_calc.calculate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive word count: 171508  Negative word count: 125916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n    Commented out as it's not running on Joshua due to issues with numpy\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Positive word count: {}  Negative word count: {}\".format(corpus_pos_neg_words_calc.get_positive_word_count(),corpus_pos_neg_words_calc.get_negative_word_count() ))\n",
    "\n",
    "'''\n",
    "    Commented out as it's not running on Joshua due to issues with numpy\n",
    "'''\n",
    "#import numpy as np\n",
    "\n",
    "# # Plot a bar graph showing the distribution\n",
    "# def plot_sentiment_graph():\n",
    "#     word_count = [corpus_pos_neg_words_calc.get_positive_word_count(), corpus_pos_neg_words_calc.get_negative_word_count()]\n",
    "#     label = [\"positive\", \"negative\"]\n",
    "#     index = np.arange(len(label))\n",
    "#     plt.bar(index, word_count)\n",
    "#     plt.xlabel('Sentiment', fontsize=10)\n",
    "#     plt.ylabel('Number of words', fontsize=10)\n",
    "#     plt.xticks(index, label, fontsize=10)\n",
    "#     plt.title('Positive and negative word counts in corpus')\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_sentiment_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B.4: Calculating the number of new stories with more positive than negative words\n",
    "\n",
    "A class **ArticleSentimentAnalyser** has been used to calculate the news stories' positive and negative words. \n",
    "\n",
    "Firstly, the class takes the file paths of the two files containing the positive and negative words. After that, the two list of words are loaded into **sets**. (Sets are used instead of list to increase performance). Additionally, there are two counters for positive and negative articles. Using the **corpus** class's iterator, all the articles are processed. \n",
    "\n",
    "For each of the article, the number of positive and negative words are calculated. If the number of positive words in the article is greater than the number of the negative words then the positive article counter is incremented. Similarly, if the number of negative words in the article is greater than the number of postive words, then the negative counter is incremented. If the number of positive and negative words are equal, nothing happens. \n",
    "\n",
    "After the processing takes places, the number of positive and negative methods can be accessed via ```analyser.get_positive_article_count()``` and ```analyser.get_negative_article_count()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "\n",
    "class ArticleSentimentAnalyser:\n",
    "    \n",
    "    def __init__(self, pos_file_path, neg_file_path, corpus):\n",
    "        self._pos_path = pos_file_path\n",
    "        self._neg_path = neg_file_path\n",
    "        \n",
    "        # Hash sets of the positive and negative words\n",
    "        self._pos_words = set()\n",
    "        self._neg_words = set()\n",
    "        \n",
    "        # Storing the positive / negative counts of each article\n",
    "        self._pos_article_count = 0\n",
    "        self._neg_article_count = 0\n",
    "        \n",
    "        # Calculated flag\n",
    "        self._calculated = False\n",
    "        \n",
    "        # Resets the pointer\n",
    "        corpus.reset_pointer()\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        words_str = \"\"\n",
    "        \n",
    "        # Read the list of positive words\n",
    "        with open(pos_file_path, \"r\") as file:\n",
    "            words_str = file.readlines()\n",
    "        \n",
    "        for word in words_str:\n",
    "            word = word.rstrip()\n",
    "            self._pos_words.add(word)\n",
    "            \n",
    "        words_str = \"\"\n",
    "        # Read the list of negative words\n",
    "        with open(neg_file_path, \"r\") as file:\n",
    "            words_str = file.readlines()\n",
    "            \n",
    "        for word in words_str:\n",
    "            word = word.rstrip()\n",
    "            self._neg_words.add(word)\n",
    "            \n",
    "    \n",
    "    def analyse_sentence(self, sentence):      \n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        \n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word in self._pos_words:\n",
    "                pos_count += 1\n",
    "            \n",
    "            if word in self._neg_words:\n",
    "                neg_count += 1\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            self._pos_article_count += 1\n",
    "        \n",
    "        if neg_count > pos_count:\n",
    "            self._neg_article_count += 1\n",
    "            \n",
    "\n",
    "    def calculate(self):\n",
    "        json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        while json_doc != None:\n",
    "            content = json_doc[\"content\"]\n",
    "            self.analyse_sentence(content)\n",
    "         \n",
    "            json_doc = self.corpus.get_next_document()\n",
    "        \n",
    "        self._calculated = True\n",
    "            \n",
    "    \n",
    "    def get_positive_article_count(self):\n",
    "        if self._calculated == False:\n",
    "            self.calculate()  \n",
    "            \n",
    "        return self._pos_article_count\n",
    "    \n",
    "    def get_negative_article_count(self):\n",
    "        if self._calculated == False:\n",
    "            self.calculate()  \n",
    "            \n",
    "        return self._neg_article_count\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the analyser, the pointer of the corpus is reset to 0. This ensures that all the articles are processed. \n",
    "\n",
    "The **ArticleSentimentAnalyser** class is instantiated with the positive, negative file paths as well as the corpus being passed in as arguments. \n",
    "\n",
    "After that, the results are printed out. \n",
    "\n",
    "**NB: In addition to printing out the results, I had created a bar graph showing the article counts but had to comment out the code as there was an issue running numpy and matplotlib.plt on Joshua**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive article count: 11044 Negative article count: 6255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n    Commented out as it's not running on Joshua due to issues with numpy\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resets the pointer\n",
    "corpus.reset_pointer()\n",
    "\n",
    "analyser = ArticleSentimentAnalyser(POSITIVE_FILE_PATH, NEGATIVE_FILE_PATH, corpus)      \n",
    "\n",
    "print(\"Positive article count: {} Negative article count: {}\".format(analyser.get_positive_article_count(), analyser.get_negative_article_count()))\n",
    "\n",
    "'''\n",
    "    Commented out as it's not running on Joshua due to issues with numpy\n",
    "'''\n",
    "# # Plot a bar graph showing the distribution\n",
    "# def plot_article_sentiment_graph():\n",
    "#     word_count = [analyser.get_positive_article_count(), analyser.get_negative_article_count()]\n",
    "#     label = [\"positive\", \"negative\"]\n",
    "#     index = np.arange(len(label))\n",
    "#     plt.bar(index, word_count)\n",
    "#     plt.xlabel('Sentiment of articles', fontsize=10)\n",
    "#     plt.ylabel('Number of articles', fontsize=10)\n",
    "#     plt.xticks(index, label, fontsize=10)\n",
    "#     plt.title('Positive and negative articles in corpus')\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_article_sentiment_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Language models \n",
    "\n",
    "Calculate the language models for trigrams based on the on the first 16,000 rows from the corpus.\n",
    "\n",
    "To do this, the trigrams, bigrams and unigrams were generated from the first 16000 rows of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 16,000 documents will be used for building up the trigrams language model\n",
    "\n",
    "\n",
    "class LanguageModelBuilder:\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        \n",
    "        # Reset corpus's pointer\n",
    "        corpus.reset_pointer()\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        self.TRAIN_UPTO = 16000\n",
    "        \n",
    "        # Uses the previously defined NGramHolder as a trigram holder\n",
    "        self.tri_holder = NGramHolder(3)\n",
    "        self.bi_holder = NGramHolder(2)\n",
    "        self.uni_holder = NGramHolder(1)\n",
    "        \n",
    "        # Flags \n",
    "        self.tri_calculated = False\n",
    "        self.bi_calculated = False\n",
    "        self.uni_calculated = False\n",
    "        \n",
    "        self.build_trigram_model()\n",
    "        self.build_bigram_model()\n",
    "        self.build_unigram_model()\n",
    "    \n",
    "    \n",
    "    def build_trigram_model(self):\n",
    "        \n",
    "        for i in range(self.TRAIN_UPTO):\n",
    "            json_doc = self.corpus.get_document(i)\n",
    "            content = json_doc[\"content\"]\n",
    "            self.tri_holder.add_n_grams_from_sentence(content)\n",
    "        \n",
    "        self.tri_calculated = True\n",
    "        \n",
    "    \n",
    "    def build_bigram_model(self):\n",
    "        \n",
    "        for i in range(self.TRAIN_UPTO):\n",
    "            json_doc = self.corpus.get_document(i)\n",
    "            content = json_doc[\"content\"]\n",
    "            self.bi_holder.add_n_grams_from_sentence(content)\n",
    "                 \n",
    "        self.bi_calculated = True\n",
    "        \n",
    "        \n",
    "    def build_unigram_model(self):\n",
    "\n",
    "        for i in range(self.TRAIN_UPTO):\n",
    "            json_doc = self.corpus.get_document(i)\n",
    "            content = json_doc[\"content\"]\n",
    "            self.uni_holder.add_n_grams_from_sentence(content)\n",
    "                 \n",
    "        self.uni_calculated = True\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.reset_pointer()\n",
    "lm = LanguageModelBuilder(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C.2: Sentence generation\n",
    "\n",
    "For generating the sentence, I am taking the last two words and searching all trigrams where those two words are the first two words. \n",
    "\n",
    "For example, if there is a phrase **\"is this\"**, all trigrams like **\"is this the\"** are returned. Getting the count of the appearance of **is this the** and dividing it by the count of the bi-gram **is this**. All the possible trigrams are divided by the count of the bi-gram to get the probabilities. The trigram for which the highest probability occurs is used. The last word in that trigram is taken and that's the next word that's added to the sentence that's being generated.\n",
    "\n",
    "**P(\"the\" | \"is this\") = P(\"is this the\") / P(\"is this\")** \n",
    "\n",
    "**P(\"the\" | \"is this\") = count(tri_grams(\"is this the\")) / count(bi_grams(\"is this\"))**\n",
    "\n",
    "In the above example, the trigrams of all different words are used instead of \"the\". The one with the highest probability is appended to the current sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is this the company has market capitalization of billion and\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_sentence(phrase, lang_mod, sent_len=10):\n",
    "    '''\n",
    "        lm is the language model.        \n",
    "    '''\n",
    "    V = vn.get_V()\n",
    "    \n",
    "    # Stop when 10 words are done \n",
    "    while len(phrase.split()) < sent_len:\n",
    "        words = phrase.split()\n",
    "        \n",
    "        second_last_word = words[len(words) - 2]\n",
    "        last_word = words[len(words) - 1]\n",
    "        \n",
    "        matching_phrases = lang_mod.tri_holder.get_n_grams_with_phrase(second_last_word + \" \" + last_word)\n",
    "        \n",
    "        max_prob = 0\n",
    "        next_word = \"\"\n",
    "        for mp in matching_phrases:\n",
    "        \n",
    "            prob = mp[1] / lang_mod.bi_holder.get_n_gram_count(second_last_word + \" \" + last_word)\n",
    "            \n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                words_in_mp = mp[0].split()\n",
    "                next_word = words_in_mp[len(words_in_mp) - 1]\n",
    "        \n",
    "        phrase += \" \" + next_word\n",
    "    \n",
    "    return phrase\n",
    "        \n",
    "print(generate_sentence(\"is this\", lm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C.2: Evaluation and Perplexity\n",
    "\n",
    "After building the language model on the first 16000 articles, it's time to see how well the models performs. This can be done by calculating the perplexity.\n",
    "\n",
    "For calculating the perplexity, the articles from 16001 to the end will be used. The perplexity will be calculated on the entire document.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FROM = 16001\n",
    "\n",
    "corpus.set_pointer(TEST_FROM)\n",
    "\n",
    "def concat_test_str(corpus, start_point=16001):\n",
    "    corpus.set_pointer(start_point)\n",
    "    \n",
    "    concat_str = \"\"\n",
    "    \n",
    "    json_doc = corpus.get_next_document()\n",
    "    while json_doc != None:    \n",
    "        concat_str += json_doc[\"content\"]\n",
    "    \n",
    "        json_doc = corpus.get_next_document()\n",
    "    \n",
    "    return concat_str\n",
    "\n",
    "test_sentence = concat_test_str(corpus, TEST_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace smoothing\n",
    "\n",
    "In order to smooth out the results for unseen ngrams in the training corpus, **Laplace smoothing** has been used. This means that 1 has been added to the numerator and the value of **V** (the vocabulary size) has been added to the denominator. It's not usually the best solution for language models due to the huge number of 0's that's replaced. However, in this case it has been used as it's very easy to implement.\n",
    "\n",
    "#### Calculating Perplexity\n",
    "\n",
    "1) To calculate the perplexity, generate bigram for the article.\n",
    "\n",
    "2) For each of the bigrams in the article, get the count for that bigram from the language model generated in part c.1. and add 1.\n",
    "\n",
    "3) For the first word in the bigram, get it's count from the unigram of the language model and add the value of the vocab size. This has been done according to the \"add-1\" estimate.\n",
    "\n",
    "4) Calculate the probability and store it in a list of probabilities\n",
    "\n",
    "5) iterate through the list of probabilities and divide 1 by the probabilities\n",
    "\n",
    "6) Iterate over this new list and multiply all the probabilities together to get the perplexity.\n",
    "\n",
    "After the perplexities for all the test articles have been calculated, they've been stored in a list called **perplexities**.\n",
    "\n",
    "For some of the perplexities, the perplexity shows up as infinite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_perplexity(lang_mod, sentence):\n",
    "\n",
    "    '''\n",
    "        lang_mod: Language model\n",
    "        setence: The whole string for rest of all the articles\n",
    "    '''\n",
    "    \n",
    "    V = vn.get_V()\n",
    "    \n",
    "    bi_holder = NGramHolder(2)\n",
    "    bi_holder.add_n_grams_from_sentence(sentence)\n",
    "    \n",
    "    bi_grams_dict = bi_holder.n_grams_dict \n",
    "    \n",
    "    probs = []\n",
    "    \n",
    "    for bi_gram, count in bi_grams_dict.items():\n",
    "        lm_bi_count = lang_mod.bi_holder.get_n_gram_count(bi_gram) + 1\n",
    "        lm_uni_count = lang_mod.uni_holder.get_n_gram_count(bi_gram.split()[0]) + V\n",
    "        \n",
    "        prob = lm_bi_count / lm_uni_count\n",
    "        probs.append(prob)\n",
    "    \n",
    "    for i in range(len(probs)):\n",
    "        probs[i] = 1 / probs[i]\n",
    "        \n",
    "\n",
    "    total_prob = float(1)\n",
    "    \n",
    "    for i in range(len(probs)):\n",
    "        total_prob *= probs[i]\n",
    "    \n",
    "    n_value = len(sentence.split())\n",
    " \n",
    "    return (total_prob**(1/ n_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the perplexity function, the perplexities for all of the remaining news articles after 16001 are calculated and stored in the perplexities list. The perplexities are printed off in the end. \n",
    "\n",
    "## The results\n",
    "\n",
    "The perplexities for some sentences are **inf**. For others they are in the hundreds or thousands. Clearly a perplexity of infinity or thousands is terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf, inf, inf, 85.22271501345406, inf, inf, inf, inf, inf, inf, inf, inf, inf, 58.72866833305018, inf, inf, inf, inf, inf, 9149.517136229177, inf, 3478.3667911379607, 773.863488487506, 4210.326414404433, inf, 11614.912859094531, inf, inf, inf, inf, inf, inf, inf, 94.32810935569726, inf, inf, inf, inf, inf, 8692.482644396674, inf, inf, inf, inf, inf, 5449.676485684123, inf, inf, inf, inf, inf, inf, inf, inf, 793.9173623628344, 6242.3983778278625, inf, inf, 228.9016369257342, inf, inf, inf, inf, 294.59226013195934, 11626.16677647464, 4097.013088150379, inf, inf, inf, inf, inf, inf, inf, inf, inf, 10300.482588569994, 2248.7158729524385, inf, inf, inf, 3475.9227231961977, inf, inf, inf, 5232.486521305419, inf, inf, inf, inf, 10113.786827451866, inf, inf, 196.62126276803642, inf, inf, inf, inf, 1764.3261575634453, inf, inf, inf, 9342.37631952564, inf, 4452.886322670646, 9275.107153992596, inf, inf, inf, inf, inf, inf, inf, inf, inf, 3955.7350140651156, inf, inf, inf, inf, inf, inf, 5574.2966357840105, inf, inf, inf, inf, inf, inf, 176.48411644901415, inf, inf, inf, 28.558861702567018, inf, inf, inf, inf, inf, 7730.356822914841, 4406.048897967686, inf, inf, inf, inf, 6008.600940279819, 5761.664391458063, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 8492.142868535033, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 7862.109388233919, inf, inf, inf, inf, 228.61022885731842, inf, 86.60090395339314, 6496.594011817611, inf, inf, inf, inf, 208.56582206316716, inf, inf, inf, inf, inf, 4063.667810859291, inf, 270.29943239045144, 123.29162064609635, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 14628.972768672958, inf, inf, inf, inf, inf, inf, 6064.275051252697, inf, inf, inf, 5470.988892260391, inf, inf, inf, inf, inf, inf, 2430.9033064045525, 6300.070976455007, inf, 1097.8959135091745, 3589.210140555124, 196.7863292506678, 6001.987368152667, inf, inf, inf, inf, inf, 9047.17347785931, 19431.872702287743, inf, inf, inf, inf, inf, 336.69505448195633, 11778.333004178665, inf, inf, inf, inf, 4219.898501276926, inf, inf, inf, inf, inf, 7014.56226666488, inf, inf, inf, 6401.238972071795, inf, inf, inf, inf, inf, inf, inf, inf, 4590.749191124323, 6754.767472702497, inf, 2118.3091136938756, inf, 4266.345062722622, 394.8758095887121, inf, 2265.059493365546, inf, inf, 5056.630127668679, inf, inf, inf, 11786.983063444404, inf, inf, inf, inf, inf, inf, inf, 137.89231931606923, inf, inf, inf, 404.11614773689246, 107.72326011921642, inf, inf, inf, inf, inf, 3958.653741946044, 705.5757429200556, inf, inf, inf, inf, inf, inf, inf, 392.67900384024364, inf, inf, 425.88995211759027, inf, inf, inf, 5034.732506801625, inf, inf, inf, inf, 610.8726513628441, 5276.3902160384905, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 668.1584235306144, inf, inf, inf, inf, 10978.127455810925, inf, 350.53299081598317, inf, inf, inf, inf, inf, 5121.15068842616, inf, inf, inf, 350.16389546483833, inf, inf, 7221.541652086305, inf, inf, inf, 238.38700922149695, inf, 8408.98251149883, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 8702.61183544576, inf, 8198.398035419596, inf, 14142.467565104304, inf, inf, inf, inf, inf, inf, 3284.080238399609, 585.5731308371963, inf, inf, 83.2108458966435, inf, inf, 2951.657272690339, inf, 6437.228637751282, inf, 6031.759749709804, inf, inf, inf, inf, inf, inf, 72.95149756788335, inf, inf, inf, inf, 27.890079760999928, inf, 23630.924812930563, inf, inf, 3749.76662415668, 128.57441863592868, 5506.955024181089, 6223.67274610046, inf, 1205.9892423648912, inf, inf, inf, inf, inf, 6906.254578172503, 14051.624203518119, inf, 827.1536756019174, inf, 3541.9059254160916, inf, 145.06377520160575, inf, inf, inf, 7448.987428391608, inf, inf, inf, inf, inf, inf, inf, 2157.0397022041834, inf, inf, inf, inf, 2774.242713618141, inf, 23835.493289198865, inf, 4589.178606200539, inf, inf, inf, inf, inf, 8465.309480392607, 11620.379826690467, inf, inf, 198.16449441475552, inf, inf, 4514.516199449096, inf, inf, inf, inf, 4594.739587057368, inf, inf, inf, 5728.292038193315, 8479.823009992038, 5400.89655340104, inf, inf, inf, 228.0633734124828, inf, inf, inf, 106.24804820610558, inf, inf, inf, inf, inf, inf, 4273.355047915975, inf, inf, inf, inf, 7511.865882788933, 467.73229638702117, inf, inf, inf, 12490.001057454394, inf, 2716.4122625426216, inf, inf, inf, inf, 196.11166946544884, 4226.9255857029375, inf, inf, inf, inf, 1604.2076263140445, inf, inf, 6540.561365597304, inf, 6246.584556488464, inf, 11990.423029844847, 10639.304070408909, 4868.5435734627945, 210.13000911429407, inf, 2616.1423945483775, 350.60291506494957, inf, 13637.87734812244, 250.9518673679846, inf, inf, 5526.974080043341, inf, inf, inf, 5786.546062804185, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 9631.944498446514, inf, 67.05236853059326, inf, inf, 6427.755937724411, inf, inf, 5477.103776261392, inf, inf, inf, inf, inf, 12042.970723101345, inf, inf, inf, inf, inf, inf, 222.44815415771455, inf, inf, 410.0626916994158, inf, 3184.7019787167856, inf, inf, inf, inf, inf, inf, inf, inf, 11860.482649224843, 342.6790030654089, inf, inf, inf, 5578.375137756761, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 786.6962576915176, inf, inf, inf, 4905.5047136544945, inf, inf, inf, 4894.9640451100995, inf, inf, inf, 216.13531511384633, inf, inf, 11081.5206913424, 123.02464171429988, inf, inf, 4887.281058210592, inf, inf, inf, inf, inf, inf, 6920.709242457685, inf, inf, inf, inf, 404.30732672420805, inf, inf, inf, inf, inf, inf, inf, inf, inf, 19426.727023332554, inf, 14990.222844216412, inf, inf, 58.72866833305018, inf, inf, inf, inf, inf, inf, 2221.984560889288, inf, inf, inf, inf, inf, inf, inf, 4757.129673456986, inf, inf, inf, inf, inf, inf, inf, inf, 2160.7274731688103, inf, inf, inf, inf, 564.3209501449221, inf, inf, inf, 237.29402665476047, 123.54456133365416, inf, inf, inf, inf, inf, inf, inf, inf, 225.7120359730008, inf, inf, inf, inf, 105.82897250694427, 129.7948488004332, 4865.903156511989, inf, inf, 1110.1474214172483, inf, 3853.9012860452767, inf, inf, inf, 9640.856401365778, inf, inf, 155.434644192176, 2015.274173024553, inf, 14971.47136172252, 2434.4649486609583, inf, inf, 3611.3915129776574, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 97.60505062199739, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 14420.54810939995, inf, inf, inf, 11533.036453407462, 1372.1212395938578, 3432.487512870964, inf, 9301.279290872233, 121.33313947694529, inf, 306.39650543210905, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 2625.103847000952, inf, inf, inf, inf, inf, inf, 5217.739034398364, inf, 10650.84879280394, inf, 211.60500513808398, inf, 52.4766889672767, inf, inf, inf, inf, inf, 8704.007285712503, inf, inf, inf, inf, inf, inf, inf, inf, 140.55347179884927, inf, inf, 118.16031854701866, inf, inf, inf, inf, inf, inf, inf, inf, 13863.700136242684, inf, inf, 585.4519206999873, inf, 50.54987960516165, inf, inf, inf, inf, inf, inf, 6602.302794697808, inf, inf, inf, inf, inf, inf, inf, inf, inf, 4182.849351551174, inf, inf, inf, inf, 4096.777758758091, inf, inf, inf, inf, inf, inf, inf, inf, 148.0112559455641, 3680.7011417085005, inf, inf, 5989.136116600908, 78.9384861984893, inf, inf, 1350.709495831053, inf, inf, inf, 213.58018183026562, inf, 12237.02125276034, inf, inf, inf, inf, inf, inf, 200.15008530078006, inf, inf, inf, inf, inf, inf, inf, inf, inf, 2151.3088319940284, inf, 8933.706427590567, inf, inf, inf, inf, inf, 9809.940872791434, inf, 9844.519536235088, inf, 95.91784182325573, inf, inf, inf, inf, inf, inf, inf, inf, 256.78788308379853, inf, inf, inf, inf, inf, inf, inf, 340.85907934342913, 5040.812221824354, inf, inf, inf, 258.90487843618683, inf, inf, 501.6335970274547, inf, 817.344666103587, inf, inf, inf, inf, inf, 551.0483023924063, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 4055.9639626899666, inf, 9502.703799166267, inf, inf, inf, inf, inf, inf, inf, 2323.6007243232298, 8542.339719625817, 131.994404039136, inf, 935.2691598348325, inf, inf, inf, inf, 26388.579425727665, inf, inf, inf, 341.47111837551546, 36.276251277149385, inf, inf, inf, inf, 767.0514553321253, 257.08692753306406, inf, inf, 5818.223345115371, inf, inf, inf, inf, 287.07797219397366, inf, 38808.672670146516, inf, inf, inf, inf, inf, inf, 353.9592024489712, 10754.651023691898, inf, inf, inf, inf, 354.91361151718627, inf, inf, inf, 4834.984495220547, 5896.987985805424, inf, 4629.361982819665, 2505.8775476511855, inf, inf, 139.48906448341884, inf, inf, inf, inf, inf, inf, 21066.712112111716, inf, inf, inf, inf, 6032.091457151531, inf, 5708.26688602691, inf, inf, inf, inf, inf, inf, 8877.166198985504, 192.2580255721061, inf, inf, 5183.642470310838, 157.65591063046438, inf, 11170.338585353004, inf, inf, inf, inf, inf, inf, 4035.846252810521, inf, inf, inf, inf, inf, inf, inf, inf, inf, 328.2363881807865, inf, inf, 6385.36064189684, inf, inf, inf, inf, 143.67956714394572, inf, inf, inf, 2780.9008854098975, inf, inf, 458.2263634197459, inf, 1003.2611373539738, 2905.011036348963, 2763.4329320563797, inf, inf, inf, inf, inf, inf, inf, 123.25534598972281, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 13466.11980552115, 381.0783537796461, 4558.858053786309, inf, inf, inf, inf, inf, inf, inf, inf, 6612.027825550515, inf, inf, inf, inf, inf, 17589.536115703308, inf, inf, inf, inf, inf, inf, 29.006900940695, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 2907.2128457628382, inf, inf, inf, inf, 351.9775783855631, inf, inf, inf, inf, inf, inf, inf, inf, inf, 888.1672150629464, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 566.7351626934259, inf, inf, inf, inf, inf, inf, inf, inf, 4097.804983557504, inf, inf, inf, inf, inf, 7389.952079915353, inf, inf, 42.13914358013681, inf, inf, inf, inf, 446.33352624933144, inf, 6091.689335918312, inf, inf, inf, inf, inf, inf, inf, inf, inf, 1890.55761863966, 6768.96416073854, inf, inf, 9684.330034775432, inf, inf, 6048.36211866089, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 301.1469680102272, inf, inf, inf, inf, inf, 326.086234286979, inf, inf, inf, 929.5004845943562, inf, inf, 1315.637017423514, inf, inf, inf, inf, inf, inf, inf, inf, inf, 194.73620383099845, inf, inf, inf, inf, inf, 152.16104829488398, inf, inf, inf, inf, inf, inf, inf, 391.8220374079862, inf, 106.54588893829184, inf, inf, 181.34481370044716, inf, inf, inf, inf, 30952.76564465532, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 5696.985229437333, inf, 107.21765858559489, inf, inf, 127.50415194331738, inf, inf, inf, inf, inf, 121.59539902181187, inf, inf, inf, inf, inf, inf, inf, inf, inf, 3135.5842742260006, inf, inf, inf, inf, inf, inf, 15096.099418989614, 3950.530582044443, inf, 5744.891925296397, inf, 366.28129190518627, inf, 451.1270047714379, inf, inf, 313.39994970540965, inf, 150.33036823530637, inf, inf, 86.54656669316333, inf, 13935.635417733401, inf, inf, inf, inf, inf, inf, 6469.833011274891, inf, inf, inf, inf, inf, 906.3780388725216, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 546.76035516435, inf, 6170.290156806213, inf, inf, inf, 6474.743985104706, inf, inf, inf, inf, inf, inf, inf, 1777.3807404506074, inf, inf, inf, inf, 484.9860829302819, inf, inf, inf, inf, inf, inf, inf, inf, 204.53851909081044, 7943.793401420271, 4611.353395958271, inf, inf, inf, inf, 7669.434385789934, inf, inf, inf, inf, 2724.123556687148, inf, inf, inf, inf, inf, 80.25066564943785, inf, inf, 7331.242657148868, inf, inf, inf, inf, inf, inf, 23.571844771382818, 15373.250722187217, 1944.8200283719852, inf, inf, inf, inf, inf, inf, 101.0508527750609, inf, inf, inf, inf, inf, inf, 3083.932205370383, inf, inf, 5053.61941955697, inf, inf, 4117.859190222147, inf, inf, inf, 78.2793794357667, 662.2480933209488, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 5551.242654777805, 12520.721211167727, 229.66152739126065, inf, inf, inf, 192.19376392425173, inf, inf, inf, inf, inf, inf, 3908.112652324706, inf, inf, inf, inf, 6908.057320369211, 4724.954747108608, inf, inf, inf, 14048.721708719402, inf, 2700.512010731504, inf, inf, 96.15269548583979, 6640.552330493154, 10278.869412969536, inf, inf, inf, inf, inf, inf, 323.35684414591776, inf, 364.3299417295077, inf, 6124.430343499714, inf, inf, inf, inf, 37.58106842031664, 835.1289156609074, 5185.620549356188, inf, inf, inf, inf, 148.63330185965526, inf, inf, inf, inf, inf, inf, inf, inf, inf, 8103.308226687336, inf, inf, inf, inf, inf, inf, inf, 116.09580077588085, inf, inf, 6110.589997146041, inf, inf, inf, inf, inf, inf, inf, 74.41558802922285, inf, 98.5100643682929, inf, inf, 15298.962054242213, 5915.787076347904, inf, inf, inf, inf, inf, inf, 4678.203643356342, inf, 5092.056618490962, inf, 6798.301348820212, inf, inf, inf, inf, inf, inf, inf, inf, 9244.23499549981, 464.93981990426124, inf, inf, inf, inf, inf, inf, inf, inf, inf, 5108.7170123968735, inf, 110.94429281057651, 13458.094535829005, 1816.0648983805615, 550.0957867256215, inf, inf, inf, 2711.451806773391, inf, inf, 6692.61782333877, inf, inf, 217.27360099257598, 604.4278361575359, 16496.763833116765, 8794.018933651723, inf, 7129.595687742161, inf, inf, inf, inf, inf, inf, inf, 132.7980079550682, 14229.211290948064, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 31.608223099152585, inf, inf, 1275.604417151334, inf, 3024.269217602559, inf, inf, inf, inf, inf, inf, inf, 195.49507838098654, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 362.1953495875555, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 3788.48330114312, inf, inf, inf, 417.59017841920524, inf, 7328.969882375434, inf, inf, inf, 5280.004463890115, inf, inf, inf, inf, 151.97433665723844, inf, inf, 7209.523848253565, 815.5536946634812, inf, inf, inf, 6061.836930461428, inf, inf, 206.56493006342703, 576.0506466048026, inf, inf, inf, inf, inf, inf, inf, inf, inf, 5262.19538890021, inf, inf, inf, inf, inf, 5076.6637489270925, inf, inf, inf, 4109.504744217752, inf, inf, inf, 7942.323041679894, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 11073.566685070542, 2436.810403388566, inf, inf, inf, inf, 247.97310862107818, inf, inf, inf, inf, inf, inf, 123.73814261574277, inf, inf, 7907.736785324311, inf, inf, inf, inf, 101.57102370482914, inf, inf, inf, inf, 161.4345423068274, inf, inf, inf, 712.1034100386596, inf, inf, 10887.137584934595, 8024.13236223703, inf, inf, inf, inf, inf, inf, inf, 168.99802411443244, inf, inf, inf, 73.75755305508117, 102.20644156330127, inf, 60.482031815354176, inf, inf, 1352.5927503014304, 12525.932747769644, inf, inf, inf, inf, 133.16250939827094, inf, inf, 8175.700165872376, inf, 63.60913517023211, inf, inf, inf, inf, inf, inf, 7078.49537589554, 377.39167794900607, inf, inf, inf, 3809.649637533031, 15169.349558868107, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 1834.8058304864721, inf, 158.32183971271112, inf, inf, 7839.870951406103, inf, 1154.6294846496512, inf, 3994.2826707741497, inf, inf, inf, inf, inf, 708.9375406232804, inf, inf, 19770.981065787786, inf, 13977.217612470728, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 348.0368040579544, 6024.033304064126, 6164.817624657528, inf, inf, 129.56392174274808, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 13335.90632020328, inf, 8457.263938151624, inf, inf, inf, 4845.381330063512, inf, inf, 772.7445721995034, inf, inf, 8568.154871353514, inf, 6039.892363948839, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 669.2393376262485, 216.9185992183689, inf, inf, inf, inf, inf, inf, 8120.875747324681, inf, inf, inf, inf, 3327.264442250825, inf, inf, inf, inf, inf, inf, inf, 254.81073725069433, inf, inf, inf, 36944.020933348416, 3653.2365579602542, inf, 2216.7834807894874, inf, 5108.963347579159, inf, inf, inf, inf, 11967.018467159069, inf, inf, inf, inf, inf, 203.52280818490092, inf, inf, 6272.1658831654895, 3024.865328398192, 3221.8462287757407, inf, 61.879400801823735, 919.8252163452969, inf, inf, inf, 539.1685216347252, inf, inf, inf, inf, inf, 2356.256924345795, inf, inf, inf, 165.04675335299163, inf, inf, 231.81741332905625, inf, inf, inf, 91.8320291405897, inf, inf, inf, 324.1145625835584, 4823.001669149533, inf, 4574.10531822729, inf, inf, 6746.555760542175, inf, 134.70488616482493, inf, inf, 7099.068762197866, 14381.52591403253, inf, inf, 5406.592663465266, 2331.1641381883783, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 11029.119090534587, 7618.371679493729, 66.85232427283032, inf, inf, 7310.561294785376, 8000.283795567622, 8345.579845787326, inf, inf, inf, 4273.94980880055, inf, 15118.143290562763, inf, inf, inf, 6821.100826660638, inf, inf, inf, inf, inf, inf, inf, 8149.25122829862, inf, 3151.2148193073003, inf, inf, inf, 47.463359695766165, inf, inf, inf, inf, 4014.4443143068343, inf, inf, inf, inf, inf, inf, 2679.9406479135673, inf, inf, inf, inf, inf, inf, inf, 3314.828070563815, 133.33458729192373, inf, inf, inf, 9756.323270265548, inf, inf, inf, inf, inf, 3720.120918140474, inf, inf, inf, inf, 786.2851056027712, inf, inf, 589.9801958709116, inf, inf, 882.0095410247527, 5312.216435560608, 122.62323359340168, inf, inf, inf, 199.50420178737332, inf, inf, inf, 9950.067072778069, 3411.452516943357, inf, inf, inf, 57.05866516302967, inf, inf, inf, inf, inf, inf, 140.82050213264108, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 12155.987152575362, inf, inf, inf, inf, inf, inf, inf, 4586.568839621646, inf, inf, inf, inf, 7426.102475185136, inf, inf, inf, inf, inf, inf, inf, inf, 12405.83724651903, inf, inf, 1261.1964661001982, 4023.260376946049, inf, inf, inf, inf, inf, inf, 7190.591354176792, inf, inf, inf, 12751.719229822866, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 6011.988699687256, inf, inf, 12206.54475942203, inf, inf, inf, 4019.8156325497375, inf, inf, inf, 6116.968300748401, 4688.265152164353, inf, inf, inf, inf, inf, 116.86964637712802, 5028.908850972909, inf, inf, inf, 2247.6496079966714, inf, inf, inf, inf, 172.4407071926206, inf, inf, 2875.148883356161, inf, inf, 6876.600858609812, inf, inf, inf, inf, inf, inf, inf, 4051.6749392244387, inf, inf, 6479.98973727438, inf, inf, 627.3372907493872, inf, 6767.717475662771, inf, inf, inf, inf, inf, inf, inf, inf, inf, 424.9108825907267, inf, 3501.081168885601, inf, inf, 7707.364511964011, 2087.595142687069, 33463.0681503986, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 414.91022878526724, inf, 4167.6084094019325, inf, inf, 127.64223734523597, inf, 862.6209974174052, inf, 6584.39094608534, 5921.403855104183, inf, inf, 71.69783428813025, inf, inf, 355.6211076878075, inf, inf, inf, inf, inf, 14177.718795891637, inf, inf, inf, inf, inf, inf, inf, 277.0026511583509, inf, inf, inf, inf, inf, inf, inf, inf, 3221.4324124102695, inf, 3919.3237726612456, 152.77211974191005, inf, inf, inf, inf, 249.8498620592907, inf, inf, inf, 2189.883410368805, inf, inf, inf, inf, 4401.486158537803, 9210.735040024101, 837.6622092737181, inf, inf, 151.0577123056445, inf, 5711.366290240872, inf, inf, inf, inf, 137.11601985364894, inf, inf, inf, inf, 2786.615668841189, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 279.46323764790077, inf, inf, inf, 5399.168398370505, inf, inf, inf, 8649.2077914173, inf, inf, inf, 197.78054001194133, 7287.000349198238, inf, inf, inf, inf, 7852.722120261846, inf, inf, inf, inf, inf, inf, 243.8656323933054, inf, inf, inf, inf, inf, inf, 152.20424841837288, inf, inf, 116.83847895962177, inf, inf, 10437.533407447509, inf, inf, 640.062918515945, inf, inf, inf, inf, inf, 7410.723019317604, inf, inf, 11118.42428503465, inf, 8832.851410939948, inf, inf, inf, inf, 6200.786115494088, inf, inf, inf, inf, inf, 2683.195677738073, inf, inf, inf, inf, 5565.911260261641, 307.62016449508116, inf, inf, inf, inf, 1038.4696635381674, inf, 4206.967021436623, inf, 165.38672384004795, inf, inf, inf, inf, inf, 468.08836570568167, inf, 8149.490035585363, inf, inf, inf, inf, inf, inf, inf, 334.7355561266788, inf, inf, inf, inf, 7051.5201701651595, inf, inf, inf, inf, inf, inf, 4515.807171472258, 10813.85238264227, inf, inf, 6638.785301057591, inf, inf, inf, inf, inf, inf, inf, 237.35929115805416, inf, 11910.569399024458, inf, 4921.034625862577, inf, 3590.529473663348, inf, inf, 2421.4523490838087, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 11303.66502831671, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 236.50677943717994, 7718.244100507351, inf, inf, 6583.526232615692, 5675.128021389769, inf, inf, inf, inf, 547.994231748323, inf, 104.43701630527829, inf, inf, inf, inf, inf, inf, 273.7172816155062, inf, inf, inf, inf, 12194.047687875438, 6857.35429108648, inf, 68.47826579273543, inf, inf, 15489.872854171392, inf, inf, inf, inf, inf, 218.6050955600498, inf, inf, inf, inf, inf, inf, 331.4002287197769, 5951.71061747187, inf, 2384.125512494481, 7716.784943815366, 9761.516497758623, inf, inf, inf, 4170.967669914859, inf, inf, inf, inf, inf, inf, 139.68368353213407, inf, inf, inf, inf, inf, inf, 5982.731842486533, 29086.25043819198, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 112.38080543934112, inf, 819.841406707945, inf, inf, inf, inf, inf, inf, 238.5585563382426, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 9397.269716381641, inf, inf, inf, inf, inf, inf, inf, inf, 12676.603589072154, inf, inf, 259.42260580476903, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 6363.914868372186, inf, inf, inf, inf, inf, inf, inf, 972.0492456001072, inf, 3500.9183228457264, inf, inf, inf, 66.00499371709235, inf, 154.36875625253003, inf, inf, inf, inf, inf, inf, 11239.351879093472, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 1548.690318672969, inf, inf, inf, inf, inf, 8398.885239125924, 12032.430183422628, inf, inf, inf, 1871.9303202443796, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 93.83531113379313, inf, inf, 207.38193784499066, inf, inf, 10971.960041248818, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 3703.5690287832417, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 825.7598119933413, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 226.26842199831336, 5079.973619157733, 438.54416814156934, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 115.48357830708774, inf, inf, inf, inf, 4499.389616188054, inf, inf, inf, inf, inf, inf, inf, 7930.151321295837, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 22946.13823882686, inf, inf, inf, inf, 1746.326073408741, inf, 13157.068140547666, inf, 2265.4652237770283, 1141.881401981171, inf, inf, inf, 5320.188935203229, 327.2174338083442, inf, inf, inf, 15707.970861782986, 41.54745112381796, inf, inf, inf, inf, 3250.4064889703236, inf, 5866.792542252426, 7902.1290494394425, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 7251.968462136143, 5400.5120605254715, inf, 3900.0393919973485, inf, 79.97769297304698, inf, inf, inf, inf, 267.9378465236685, inf, 3576.8969025002234, inf, 5824.76813145657, inf, inf, inf, inf, 5506.383153799258, inf, 4269.980388912533, inf, inf, inf, inf, inf, inf, 5783.97478780466, inf, 209.0123968128855, inf, inf, inf, 97.48593459012939, 2561.9181681487366, inf, 6623.687400921467, inf, inf, 7834.890241010272, inf, inf, inf, 5760.996921590425, inf, 7159.686850382909, inf, 6991.168243660658, inf, inf, inf, 6038.033967691362, inf, 4907.17233658392, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 142.39828400093742, 74.123922936309, inf, inf, inf, inf, inf, inf, inf, inf, 75.18599310173293, inf, inf, inf, inf, 149.34162584863816, inf, 113.1224146206659, inf, inf, inf, inf, inf, inf, inf, 4834.415958189368, inf, inf, inf, inf, inf, inf, inf, 274.556803072092, 48.86985868997519, inf, inf, inf, inf, inf, 4981.939091377665, inf, inf, inf, inf, 14590.475013792086, inf, inf, 8743.163786794848, inf, inf, inf, inf, inf, inf, inf, 14918.724170302201, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 8688.400762761208, inf, inf, inf, 3811.662757727277, 63.579584195332096, inf, inf, inf, 2264.3471947526828, inf, 12373.719069630682, 2515.0489809379947, inf, inf, inf, inf, inf, 4015.8744534699335, inf, 2120.50243330933, 2749.296431353029, inf, inf, inf, inf, inf, inf, 1978.2666911921604, inf, inf, 157.92109256162385, inf, 387.44205023743086, inf, inf, 380.0598111656184, 3117.1460974923443, inf, inf, 4787.993052992256, inf, inf, inf, inf, inf, inf, inf, inf, 259.4884328317522, inf, inf, inf, inf, inf, inf, inf, inf, inf, 2859.86346998255, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 4326.470034594574, inf, inf, inf, inf, inf, 1240.272140401317, inf, inf, inf, inf, inf, inf, inf, inf, 444.89561808616753, inf, inf, inf, inf, 3004.278764336716, inf, inf, inf, 94.88846800257937, inf, 4201.391001002638, inf, inf, inf, inf, 4308.243215688727, 14756.889989874337, inf, inf, inf, 501.25725569183174, inf, inf, inf, inf, inf, inf, inf, inf, inf, 6087.970362646127, inf, inf, inf, 2670.981528094431, inf, 4151.89574214341, 8822.145483161175, inf, inf, inf, inf, inf, 889.1419267537241, inf, inf, 4065.686349435627, inf, 11557.862973154595, 201.73287705357984, 3384.7325652207664, inf, inf, inf, inf, inf, 9017.368733744577, inf, 394.556584423985, inf, inf, 5065.78849355107, 482.1509778037215, inf, inf, 114.69357395160135, inf, inf, 297.6527115603486, 5438.038880642401, inf, inf, inf, inf, inf, 4853.9278483309645, inf, inf, inf, inf, inf, inf, inf, 776.2898417540586, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 231.32284144550817, inf, inf, 150.41118763377764, inf, inf, inf, inf, inf, 7698.234117188249, inf, 3537.0607541357817, inf, inf, 282.21560100526926, inf, inf, inf, inf, 10932.578029787068, inf, inf, 7969.976042331296, 377.18760242950225, inf, inf, inf, inf, 98.81340679336601, inf, 278.3437460615277, inf, inf, 1363.523066870747, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 520.0303974187364, inf, inf, inf, 3084.3694703987635, inf, inf, inf, inf, 5121.417299811892, 4025.2801892561133, inf, 2396.950565493633, inf, 8296.466224987125, inf, inf, 4757.913046669813, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 100.16626092281983, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 2780.771691046637, inf, 2055.8602316239794, 260.2435707180983, inf, inf, 172.9768107295987, 2162.7924936903164, inf, inf]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_perplexities(corpus, lang_mod):\n",
    "    perplexities = []\n",
    "    corpus.set_pointer(TEST_FROM)\n",
    "\n",
    "    json_doc = corpus.get_next_document()\n",
    "    while json_doc != None:\n",
    "        \n",
    "        content = json_doc[\"content\"]\n",
    "        perplexities.append(calculate_perplexity(lm, content))\n",
    "        \n",
    "        json_doc = corpus.get_next_document()\n",
    "    \n",
    "    return perplexities\n",
    "\n",
    "\n",
    "perplexities = calculate_perplexities(corpus, lm)\n",
    "        \n",
    "print(perplexities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
