{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Path\n",
    "\n",
    "The code cell below is used for setting the NLTK path on DCS machines and Joshua. On my personal computer where this assignment was done, I had downloaded the NLTK corpus using **nltk.download()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "if username is not \"ArghaWin10\" or username is not \"arghasarkar\":\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    \n",
    "    nltk.data.path.append('/modules/cs918/nltk_data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "\n",
    "Sanity check to ensure that the file exists before proceeding further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import testsets\n",
    "import evaluation\n",
    "\n",
    "from os.path import normpath\n",
    "import os\n",
    "\n",
    "# TODO: load training data\n",
    "FILE_NAME = \"./twitter-dev-data.txt\"\n",
    "\n",
    "if os.path.isfile(FILE_NAME):\n",
    "    print(\"The file has been found.\")\n",
    "else:\n",
    "    raise IOError(\"File not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentiment marked lexicons\n",
    "\n",
    "Using lexicons marked with a value indicating positive / neutral / negative sentiment. These words and their counts in tweets will be used for checking the sentiment of the tweets. This is only as a basic classifier. \n",
    "\n",
    "The format of the file is like: \n",
    "\n",
    "**word{tab}sentiment_value**\n",
    "\n",
    "The sentiment value is between +1 to -1 and are real numbers. \n",
    "\n",
    "- +1 is extremely positive\n",
    "- -1 is extremely negative\n",
    "- Everything in between is on the postive / negative spectrum.\n",
    "\n",
    "Using a dictionary where the **word is the key** and **the value is their rating**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words sentiment file has been found.\n",
      "Positive count: 0 Negative count:  0\n"
     ]
    }
   ],
   "source": [
    "WORDS_FILE_NAME = \"wordsWithStrength.txt\"\n",
    "\n",
    "class WordSentiment:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.pos_words = {}\n",
    "        self.neg_words = {}\n",
    "        self.all_words = {}\n",
    "        \n",
    "        self.load_words()\n",
    "    \n",
    "    \n",
    "    def load_words(self):\n",
    "\n",
    "        if os.path.isfile(FILE_NAME):\n",
    "            print(\"The words sentiment file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found: Words Sentiment.\")\n",
    "        \n",
    "        with open(self.file_name, \"r\") as corpus:\n",
    "            data = corpus.readlines()\n",
    "            self.process_words(data)\n",
    "            \n",
    "            \n",
    "    def process_words(self, data):\n",
    "        \n",
    "        for line in data:\n",
    "            split_line = line.split(\"\\t\") \n",
    "            _word = split_line[0].lower()\n",
    "            _score = split_line[1]\n",
    "            _score = float(_score)\n",
    "            \n",
    "#             if _score > 0:\n",
    "#                 self.pos_words[_word] = _score\n",
    "#             else:\n",
    "#                 self.neg_words[_word] = _score      \n",
    "                \n",
    "            self.all_words[_word] = _score\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def print_words_summary(self):   \n",
    "        print(\"Positive count: {} Negative count: \".format(str(len(self.pos_words.keys()))), str(len(self.neg_words.keys())))\n",
    "        \n",
    "\n",
    "ws = WordSentiment(WORDS_FILE_NAME)\n",
    "ws.print_words_summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet class\n",
    "\n",
    "The Tweet class is passed on a single line from the training dataset. It then parses it to get the ```tweet_id```, sentiment and the actual tweet. \n",
    "\n",
    "Additionally, it is also responsible for performing the preprocessing required during the instantiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ID: 735752723159607191 -- Sentiment: positive -- Tweet: shay with bentley and bella in their sunday best urllink happyface urllink hello \n",
      " Tokens: ['shay', 'with', 'bentley', 'and', 'bella', 'in', 'their', 'sunday', 'best', 'urllink', 'happyface', 'urllink', 'hello']\n",
      "\n",
      "Tweet ID: 529243425878060644 -- Sentiment: negative -- Tweet: dear msm cnn bitches every election one turning point there you go again will not use my opponent youth now basket of deplorables \n",
      " Tokens: ['dear', 'msm', 'cnn', 'bitches', 'every', 'election', 'one', 'turning', 'point', 'there', 'you', 'go', 'again', 'will', 'not', 'use', 'my', 'opponent', 'youth', 'now', 'basket', 'of', 'deplorables']\n",
      "\n",
      "Tweet ID: 348472267247705036 -- Sentiment: negative -- Tweet: usermention chip defines child at conception some democrat want to end chip by folding it into medicaid should urllink \n",
      " Tokens: ['usermention', 'chip', 'defines', 'child', 'at', 'conception', 'some', 'democrat', 'want', 'to', 'end', 'chip', 'by', 'folding', 'it', 'into', 'medicaid', 'should', 'urllink']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "class Tweet:\n",
    "    \n",
    "    def __init__(self, raw_str, lemmatizer):\n",
    "        \n",
    "        self.raw_str = raw_str\n",
    "        self.id, self.sentiment, self.tweet = self.parse_raw_tweet()\n",
    "        \n",
    "        self.parsed_tweet = \"\"\n",
    "        self.tokens = []\n",
    "        \n",
    "        self.lemmatizer = lemmatizer\n",
    "        \n",
    "        # Methods to run\n",
    "        self.preprocess_tweet()\n",
    "        \n",
    "        # Post tokenized string\n",
    "        self.tok_str = \"\"\n",
    "        # Tokenize preprocessed tweet\n",
    "        self.tokenize()\n",
    "        \n",
    "    \n",
    "    def parse_raw_tweet(self):\n",
    "        parts = self.raw_str.split(\"\\t\")\n",
    "        return parts[0], parts[1], parts[2]\n",
    "    \n",
    "    \n",
    "    def preprocess_tweet(self):\n",
    "        \n",
    "        # Lower case the string\n",
    "        self.tweet = self.tweet.lower()\n",
    "        \n",
    "        # Replace URLs\n",
    "        self._preprocess_replace_URLs()\n",
    "        \n",
    "        # Replace user mentions\n",
    "        self._preprocess_replace_user_mentions()\n",
    "        \n",
    "        # Remove one character long words\n",
    "        self._preprocess_remove_one_char_long_words()\n",
    "        \n",
    "        # Substitute emojis\n",
    "        self._preprocess_substitute_emojis()\n",
    "        \n",
    "        # Lemmatize\n",
    "        self._preprocess_lemmatize()\n",
    "        \n",
    "        # Remove non-alphanumeric characters\n",
    "        self._preprocess_remove_all_non_alphanumeric_chars()\n",
    "    \n",
    "    \n",
    "    def _preprocess_replace_URLs(self):  \n",
    "        self.tweet = re.sub(r'\\b(http)(s)?:\\/\\/((([\\w\\/])(\\.)?)+)\\b', 'urllink', self.tweet)\n",
    "    \n",
    "    \n",
    "    def _preprocess_replace_user_mentions(self):\n",
    "        self.tweet = re.sub(r'^(?!.*\\bRT\\b)(?:.+\\s)?@\\w+', 'usermention', self.tweet)\n",
    "        \n",
    "        \n",
    "    def _preprocess_remove_one_char_long_words(self):\n",
    "        self.tweet = re.sub(r'\\b[A-Za-z0-9]{1}\\b', ' ', self.tweet)\n",
    "        \n",
    "    \n",
    "    def _preprocess_substitute_emojis(self):\n",
    "        self.tweet = re.sub(r':\\)|:]|:3|:>|8\\)|\\(:|=\\)|=]|:\\'\\)', 'happyface', self.tweet)\n",
    "        self.tweet = re.sub(r':\\(|:\\[|:<|8\\(|\\(:|=\\(|=\\[|:\\'\\(|:-\\(', 'sadface', self.tweet)\n",
    "\n",
    "    \n",
    "    def _preprocess_remove_all_non_alphanumeric_chars(self):\n",
    "        self.tweet = re.sub(r'[^\\sa-zA-Z0-9]', ' ', self.tweet)\n",
    "    \n",
    "    \n",
    "    def _preprocess_lemmatize(self):\n",
    "\n",
    "        lemmatizer = self.lemmatizer\n",
    "        words = self.tweet.split()\n",
    "        for i in range(len(words)):\n",
    "            words[i] = lemmatizer.lemmatize(words[i])\n",
    "            \n",
    "        self.tweet =  \" \".join(words)\n",
    "    \n",
    "    \n",
    "    def tokenize(self):\n",
    "        self.tokens = tokenizer.tokenize(self.tweet)\n",
    "        self.tok_str = \" \".join(self.tokens)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\"Tweet ID: {} -- Sentiment: {} -- Tweet: {} \\n Tokens: {}\\n\".format(str(self.id), self.sentiment, self.tok_str, self.tokens))\n",
    "    \n",
    "\n",
    "    \n",
    "# # ''' \n",
    "# # Test code below. IGNORE -------\n",
    "# # '''\n",
    "\n",
    "wnLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "rt1 = \"735752723159607191\tpositive\tShay with Bentley and Bella, in their sunday best https://google.co.uk :) http://t.co/SUMZBSTrkW hello\"\n",
    "rt2 = \"529243425878060644\tnegative\tDear MSM, CNN bitches, every election one turning point..there you go again I will not use my opponents youth & NOW \\\"basket of deplorables\\\"\"\n",
    "rt3 = \"348472267247705036\tnegative\t@LifeNewsHQ CHIP defines a child at conception. Some Democrats want to end CHIP by folding it into Medicaid. Should… https://t.co/To21fCSHkO\"\n",
    "\n",
    "t1 = Tweet(rt1, wnLemmatizer)\n",
    "t2 = Tweet(rt2, wnLemmatizer)\n",
    "t3 = Tweet(rt3, wnLemmatizer)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus \n",
    "\n",
    "Corpus object to read the data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n",
      "Number of training samples: 2000.\n",
      "Number of positive samples: 703.\n",
      "Number of neutral samples: 919.\n",
      "Number of negative samples: 378.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, file_path, lemmatizer = WordNetLemmatizer()):\n",
    "        # Private vars\n",
    "        self._corpus_loaded = False\n",
    "        self.lemmatizer = lemmatizer\n",
    "        \n",
    "        self.file_path = file_path \n",
    "        \n",
    "        # Checks if the file exists\n",
    "        if os.path.isfile(self.file_path):\n",
    "            print(\"The file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found.\")\n",
    "        \n",
    "        self.raw_docs = []\n",
    "        \n",
    "        # Contains all the tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.processed_dict = {}\n",
    "        \n",
    "        # Contains all the positive tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.pos_tweets = {}\n",
    "        \n",
    "        # Contains all the neutral tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.neu_tweets = {}\n",
    "            \n",
    "        # Contains all the negativve tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.neg_tweets = {}\n",
    "        \n",
    "    \n",
    "    def load_corpus(self):\n",
    "        \n",
    "        with open(self.file_path, \"r\", encoding=\"utf8\") as corpus:\n",
    "            data = corpus.readlines()\n",
    "            for line in data:\n",
    "                self.raw_docs.append(line)\n",
    "        \n",
    "        self._corpus_loaded = True\n",
    "    \n",
    "    \n",
    "    def parse_corpus(self):\n",
    "        \n",
    "        for line in self.raw_docs:\n",
    "            _tweet = Tweet(line, self.lemmatizer)\n",
    "            _sentiment = _tweet.sentiment\n",
    "            _id = _tweet.id\n",
    "            \n",
    "            if _sentiment == \"positive\":\n",
    "                self.pos_tweets[_id] = _tweet\n",
    "            elif _sentiment == \"neutral\": \n",
    "                self.neu_tweets[_id] = _tweet\n",
    "            else:\n",
    "                self.neg_tweets[_id] = _tweet\n",
    "            \n",
    "            self.processed_dict[_id] = _tweet\n",
    "                \n",
    "        self._corpus_parsed = True\n",
    "    \n",
    "    \n",
    "    def print_summary_of_corpus(self):\n",
    "        \n",
    "        if not self._corpus_loaded:\n",
    "            self.load_corpus()\n",
    "        \n",
    "        if not self._corpus_parsed:\n",
    "            self.parse_corpus()\n",
    "                      \n",
    "        print(\"Number of training samples: {}.\".format(str(len(self.raw_docs))))\n",
    "        print(\"Number of positive samples: {}.\".format(str(len(self.pos_tweets))))\n",
    "        print(\"Number of neutral samples: {}.\".format(str(len(self.neu_tweets))))\n",
    "        print(\"Number of negative samples: {}.\".format(str(len(self.neg_tweets))))\n",
    "        \n",
    "    \n",
    "    def print_positive_tweets(self):\n",
    "        \n",
    "        for _id in self.pos_tweets:\n",
    "            print(self.pos_tweets[_id])\n",
    "            \n",
    "            \n",
    "    def print_neutral_tweets(self):\n",
    "    \n",
    "        for _id in self.neu_tweets:\n",
    "            print(self.neu_tweets[_id])\n",
    "            \n",
    "            \n",
    "    def print_negative_tweets(self):\n",
    "        \n",
    "        for _id in self.neg_tweets:\n",
    "            print(self.neg_tweets[_id])\n",
    "        \n",
    "\n",
    "c = Corpus(FILE_NAME)\n",
    "c.load_corpus()\n",
    "c.parse_corpus()\n",
    "c.print_summary_of_corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a count vector\n",
    "\n",
    "For the second classifier, a count vector is required. Before that can be done, a list of all tweets is needed. So first all, need to go through all the tweets in the corpus and build a new list of just tweet strings. After building the list of tweet strings, use ```CountVectorizer()``` to transform it into a vector by using the ```vectorizer.fit_transform(corpus)``` function. Whilst doing this, we need to also keep track of the labels of each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literally so excited going to sam smith concert in october\n",
      "positive\n",
      "  (0, 11)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class CountVectorCorpus:\n",
    "    \n",
    "    def __init__(self, original_corpus):\n",
    "        \n",
    "        self.vectorizer = CountVectorizer()\n",
    "        \n",
    "        self.original_corpus = original_corpus\n",
    "        \n",
    "        self.tweets_list, self.labels_list, self.vector_corpus = self.build_list()\n",
    "        \n",
    "\n",
    "    def build_list(self):\n",
    "        tweets = self.original_corpus.processed_dict\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for _id in tweets:\n",
    "            tweet = tweets[_id]\n",
    "            _sentiment = tweet.sentiment\n",
    "            x_list.append(tweet.tok_str)\n",
    "            y_list.append(_sentiment)\n",
    "            \n",
    "        vect_corp = self.vectorizer.fit_transform(x_list[:2])\n",
    "        \n",
    "        print(x_list[1])\n",
    "        print(y_list[1])\n",
    "        print(vect_corp[1])\n",
    "        return x_list, y_list, vect_corp\n",
    "            \n",
    "\n",
    "cvc = CountVectorCorpus(c)\n",
    "# print(cvc.vector_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon classifier class\n",
    "\n",
    "This classifier uses the word's positive and negative ratings to work out the overall sentiment. The method ```classify_tweet(Tweet)``` takes the Tweet class as an argument. It extracts the Tweet's text and then classifies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexiconClassifier: \n",
    "    \n",
    "    def __init__(self, word_sentiment):\n",
    "        \n",
    "        self.min_neut = -0.15\n",
    "        self.max_neut = 0.15\n",
    "        \n",
    "        \n",
    "        self.word_sentiment = word_sentiment\n",
    "        \n",
    "        \n",
    "    def classify_tweet(self, tweet):\n",
    "        \n",
    "        words_dict = self.word_sentiment.all_words   \n",
    "        score = float(0)\n",
    "        tokens = tweet.tokens\n",
    "    \n",
    "        sentiment = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            if tok in words_dict:\n",
    "                score += words_dict[tok]\n",
    "        \n",
    "        if score > self.min_neut and score < self.max_neut:\n",
    "            sentiment = \"neutral\"\n",
    "        elif score > self.max_neut:\n",
    "            sentiment = \"positive\"\n",
    "        else:\n",
    "            sentiment = \"negative\"\n",
    "\n",
    "            \n",
    "        return score, str(sentiment)\n",
    "\n",
    "\n",
    "# Declaring the lexicon Classifier\n",
    "lc = LexiconClassifier(ws)\n",
    "# print(lc.classify_tweet(Tweet(\"382489758445350006\tnegative\thigh prozac\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loader class\n",
    "\n",
    "Class for loading the test sets. It accepts the **file_name** and the **classifier** as arguments. It returns the dictionary of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestData:\n",
    "    \n",
    "    def __init__(self, file_name, corpus, classifier):\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        self.corpus = corpus\n",
    "        self.classifier = classifier   \n",
    "        \n",
    "        self.classified_dict = {}\n",
    "\n",
    "    def run_classifier(self):\n",
    "        \n",
    "        # Predictions dictionary\n",
    "        results_dict = {}\n",
    "        \n",
    "        # Tweets dict\n",
    "        td = self.corpus.processed_dict\n",
    "        \n",
    "        for _id in td:\n",
    "            _tweet = td[_id]\n",
    "            classification = self.classifier.classify_tweet(_tweet)\n",
    "            results_dict[str(_id)] = classification[1]\n",
    "        \n",
    "        return results_dict\n",
    "\n",
    "    \n",
    "# td1 = TestData(testsets.testsets[0], c, lc)\n",
    "# td1.run_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lex_classifier\n",
      "The file has been found.\n",
      "twitter-dev-data.txt (lex_classifier): 0.400\n",
      "            positive  negative  neutral\n",
      "positive    0.448     0.155     0.397     \n",
      "negative    0.253     0.234     0.513     \n",
      "neutral     0.307     0.190     0.503     \n",
      "\n",
      "Training myclassifier2\n",
      "twitter-dev-data.txt (myclassifier2): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.351     0.189     0.460     \n",
      "\n",
      "Training myclassifier3\n",
      "twitter-dev-data.txt (myclassifier3): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.351     0.189     0.460     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for classifier in ['lex_classifier', 'myclassifier2', 'myclassifier3']: # You may rename the names of the classifiers to something more descriptive\n",
    "\n",
    "    valFound = False\n",
    "    \n",
    "    if classifier == 'lex_classifier':\n",
    "        print('Training ' + classifier)\n",
    "        # TODO: extract features for training classifier1\n",
    "        # TODO: train sentiment classifier1\n",
    "    elif classifier == 'myclassifier2':\n",
    "        print('Training ' + classifier)\n",
    "        # TODO: extract features for training classifier2\n",
    "        # TODO: train sentiment classifier2\n",
    "    elif classifier == 'myclassifier3':\n",
    "        print('Training ' + classifier)\n",
    "        # TODO: extract features for training classifier3\n",
    "        # TODO: train sentiment classifier3\n",
    "\n",
    "    for testset in testsets.testsets:\n",
    "        # TODO: classify tweets in test set\n",
    "\n",
    "        predictions = {'163361196206957578': 'neutral', '768006053969268950': 'neutral', '742616104384772304': 'neutral', '102313285628711403': 'neutral', '653274888624828198': 'neutral'} # TODO: Remove this line, 'predictions' should be populated with the outputs of your classifier\n",
    "        \n",
    "        if classifier == 'lex_classifier':\n",
    "            test_corpus = Corpus(testset)\n",
    "            test_corpus.load_corpus()\n",
    "            test_corpus.parse_corpus()\n",
    "\n",
    "            td1 = TestData(testset, test_corpus, lc)\n",
    "            predictions = td1.run_classifier()\n",
    "            \n",
    "        elif classifier == 'myclassifier2':\n",
    "            pass\n",
    "        elif classifier == 'myclassifier3':\n",
    "            pass\n",
    "#         print(predictions)\n",
    "#         predictions = {}\n",
    "\n",
    "        \n",
    "        evaluation.evaluate(predictions, testset, classifier)\n",
    "\n",
    "        evaluation.confusion(predictions, testset, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@LifeNewsHQ',\n",
       " 'CHIP',\n",
       " 'defines',\n",
       " 'a',\n",
       " 'child',\n",
       " 'at',\n",
       " 'conception',\n",
       " '.',\n",
       " 'Some',\n",
       " 'Democrats',\n",
       " 'want',\n",
       " 'to',\n",
       " 'end',\n",
       " 'CHIP',\n",
       " 'by',\n",
       " 'folding',\n",
       " 'it',\n",
       " 'into',\n",
       " 'Medicaid',\n",
       " '.',\n",
       " 'Should',\n",
       " '…',\n",
       " 'https://t.co/To21fCSHkO']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TweetTokenizer()\n",
    "\n",
    "t1 = \"\\\"#Obergefell, Marriage Equality and Islam in the West http://t.co/NoQlB3g6t0 #IslaminAmerica #marriageequality\\\"\"\n",
    "t2 = \"@LifeNewsHQ CHIP defines a child at conception. Some Democrats want to end CHIP by folding it into Medicaid. Should… https://t.co/To21fCSHkO\"\n",
    "\n",
    "tok.tokenize(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle\n",
      "['hello', 'world', 'from', 'argha']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "list_pkl_name = \"parrot.pkl\"\n",
    "\n",
    "my_list = []\n",
    "\n",
    "if os.path.isfile(list_pkl_name):\n",
    "    # Files exists so read it\n",
    "    print(\"Reading pickle\")\n",
    "    with open(list_pkl_name, 'rb') as f:\n",
    "        my_list = pickle.load(f)\n",
    "else:\n",
    "    # Create it and save it\n",
    "    print(\"writing pickle\")\n",
    "    my_list = [\"hello\", \"world\", \"from\", \"argha\"]\n",
    "    with open(list_pkl_name, 'wb') as f:\n",
    "        pickle.dump(my_list, f)\n",
    "        \n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hi', 'world']\n",
      "[[1 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "corpus = [\n",
    "    \"Hello World\",\n",
    "    \"Hi World\",\n",
    "    \"World Hello\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
