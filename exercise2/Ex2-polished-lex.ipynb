{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Path\n",
    "\n",
    "The code cell below is used for setting the NLTK path on DCS machines and Joshua. On my personal computer where this assignment was done, I had downloaded the NLTK corpus using **nltk.download()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "if username is not \"ArghaWin10\" or username is not \"arghasarkar\":\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    \n",
    "    nltk.data.path.append('/modules/cs918/nltk_data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "\n",
    "Sanity check to ensure that the file exists before proceeding further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import testsets\n",
    "import evaluation\n",
    "\n",
    "from os.path import normpath\n",
    "import os\n",
    "\n",
    "# TODO: load training data\n",
    "FILE_NAME = \"./twitter-training-data.txt\"\n",
    "\n",
    "if os.path.isfile(FILE_NAME):\n",
    "    print(\"The file has been found.\")\n",
    "else:\n",
    "    raise IOError(\"File not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentiment marked lexicons\n",
    "\n",
    "Using lexicons marked with a value indicating positive / neutral / negative sentiment. These words and their counts in tweets will be used for checking the sentiment of the tweets. This is only as a basic classifier. \n",
    "\n",
    "The format of the file is like: \n",
    "\n",
    "**word{tab}sentiment_value**\n",
    "\n",
    "The sentiment value is between +1 to -1 and are real numbers. \n",
    "\n",
    "- +1 is extremely positive\n",
    "- -1 is extremely negative\n",
    "- Everything in between is on the postive / negative spectrum.\n",
    "\n",
    "Using a dictionary where the **word is the key** and **the value is their rating**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words sentiment file has been found.\n"
     ]
    }
   ],
   "source": [
    "WORDS_FILE_NAME = \"wordsWithStrength.txt\"\n",
    "\n",
    "class WordSentiment:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.pos_words = {}\n",
    "        self.neg_words = {}\n",
    "        self.all_words = {}\n",
    "        \n",
    "        self.load_words()\n",
    "    \n",
    "    \n",
    "    def load_words(self):\n",
    "\n",
    "        if os.path.isfile(FILE_NAME):\n",
    "            print(\"The words sentiment file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found: Words Sentiment.\")\n",
    "        \n",
    "        with open(self.file_name, \"r\") as corpus:\n",
    "            data = corpus.readlines()\n",
    "            self.process_words(data)\n",
    "            \n",
    "            \n",
    "    def process_words(self, data):\n",
    "        \n",
    "        for line in data:\n",
    "            split_line = line.split(\"\\t\") \n",
    "            _word = split_line[0].lower()\n",
    "            _score = split_line[1]\n",
    "            _score = float(_score)\n",
    "                 \n",
    "            self.all_words[_word] = _score\n",
    "        \n",
    "        \n",
    "    def print_words_summary(self):   \n",
    "        print(\"Positive count: {} Negative count: \".format(str(len(self.pos_words.keys()))), str(len(self.neg_words.keys())))\n",
    "        \n",
    "\n",
    "ws = WordSentiment(WORDS_FILE_NAME)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet class\n",
    "\n",
    "The Tweet class is passed on a single line from the training dataset. It then parses it to get the ```tweet_id```, sentiment and the actual tweet. \n",
    "\n",
    "Additionally, it is also responsible for performing the preprocessing required during the instantiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "class Tweet:\n",
    "    \n",
    "    def __init__(self, raw_str, lemmatizer):\n",
    "        \n",
    "        self.raw_str = raw_str\n",
    "        self.id, self.sentiment, self.tweet = self.parse_raw_tweet()\n",
    "        \n",
    "        self.parsed_tweet = \"\"\n",
    "        self.tokens = []\n",
    "        \n",
    "        self.lemmatizer = lemmatizer\n",
    "        \n",
    "        # Methods to run\n",
    "        self.preprocess_tweet()\n",
    "        \n",
    "        # Post tokenized string\n",
    "        self.tok_str = \"\"\n",
    "        # Tokenize preprocessed tweet\n",
    "        self.tokenize()\n",
    "        \n",
    "    \n",
    "    def parse_raw_tweet(self):\n",
    "        parts = self.raw_str.split(\"\\t\")\n",
    "        return parts[0], parts[1], parts[2]\n",
    "    \n",
    "    \n",
    "    def preprocess_tweet(self):\n",
    "        \n",
    "        # Lower case the string\n",
    "        self.tweet = self.tweet.lower()\n",
    "        \n",
    "        # Replace URLs\n",
    "        self._preprocess_replace_URLs()\n",
    "        \n",
    "        # Replace user mentions\n",
    "        self._preprocess_replace_user_mentions()\n",
    "        \n",
    "        # Remove one character long words\n",
    "        self._preprocess_remove_one_char_long_words()\n",
    "        \n",
    "        # Substitute emojis\n",
    "        self._preprocess_substitute_emojis()\n",
    "        \n",
    "        # Lemmatize\n",
    "        self._preprocess_lemmatize()\n",
    "        \n",
    "        # Remove non-alphanumeric characters\n",
    "        self._preprocess_remove_all_non_alphanumeric_chars()\n",
    "    \n",
    "    \n",
    "    def _preprocess_replace_URLs(self):  \n",
    "        self.tweet = re.sub(r'\\b(http)(s)?:\\/\\/((([\\w\\/])(\\.)?)+)\\b', 'urllink', self.tweet)\n",
    "    \n",
    "    \n",
    "    def _preprocess_replace_user_mentions(self):\n",
    "        self.tweet = re.sub(r'^(?!.*\\bRT\\b)(?:.+\\s)?@\\w+', 'usermention', self.tweet)\n",
    "        \n",
    "        \n",
    "    def _preprocess_remove_one_char_long_words(self):\n",
    "        self.tweet = re.sub(r'\\b[A-Za-z0-9]{1}\\b', ' ', self.tweet)\n",
    "        \n",
    "    \n",
    "    def _preprocess_substitute_emojis(self):\n",
    "        self.tweet = re.sub(r':\\)|:]|:3|:>|8\\)|\\(:|=\\)|=]|:\\'\\)', 'happyface', self.tweet)\n",
    "        self.tweet = re.sub(r':\\(|:\\[|:<|8\\(|\\(:|=\\(|=\\[|:\\'\\(|:-\\(', 'sadface', self.tweet)\n",
    "\n",
    "    \n",
    "    def _preprocess_remove_all_non_alphanumeric_chars(self):\n",
    "        self.tweet = re.sub(r'[^\\sa-zA-Z0-9]', ' ', self.tweet)\n",
    "    \n",
    "    \n",
    "    def _preprocess_lemmatize(self):\n",
    "\n",
    "        lemmatizer = self.lemmatizer\n",
    "        words = self.tweet.split()\n",
    "        for i in range(len(words)):\n",
    "            words[i] = lemmatizer.lemmatize(words[i])\n",
    "            \n",
    "        self.tweet =  \" \".join(words)\n",
    "    \n",
    "    \n",
    "    def tokenize(self):\n",
    "        self.tokens = tokenizer.tokenize(self.tweet)\n",
    "        self.tok_str = \" \".join(self.tokens)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\"Tweet ID: {} -- Sentiment: {} -- Tweet: {} \\n Tokens: {}\\n\".format(str(self.id), self.sentiment, self.tok_str, self.tokens))\n",
    "    \n",
    "\n",
    "    \n",
    "# # ''' \n",
    "# # Test code below. IGNORE -------\n",
    "# # '''\n",
    "\n",
    "wnLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# rt1 = \"735752723159607191\tpositive\tShay with Bentley and Bella, in their sunday best https://google.co.uk :) http://t.co/SUMZBSTrkW hello\"\n",
    "# rt2 = \"529243425878060644\tnegative\tDear MSM, CNN bitches, every election one turning point..there you go again I will not use my opponents youth & NOW \\\"basket of deplorables\\\"\"\n",
    "# rt3 = \"348472267247705036\tnegative\t@LifeNewsHQ CHIP defines a child at conception. Some Democrats want to end CHIP by folding it into Medicaid. Shouldâ€¦ https://t.co/To21fCSHkO\"\n",
    "\n",
    "# t1 = Tweet(rt1, wnLemmatizer)\n",
    "# t2 = Tweet(rt2, wnLemmatizer)\n",
    "# t3 = Tweet(rt3, wnLemmatizer)\n",
    "\n",
    "# print(t1)\n",
    "# print(t2)\n",
    "# print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus \n",
    "\n",
    "Corpus object to read the data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been found.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, file_path, lemmatizer = WordNetLemmatizer()):\n",
    "        # Private vars\n",
    "        self._corpus_loaded = False\n",
    "        self.lemmatizer = lemmatizer\n",
    "        \n",
    "        self.file_path = file_path \n",
    "        \n",
    "        # Checks if the file exists\n",
    "        if os.path.isfile(self.file_path):\n",
    "            print(\"The file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found.\")\n",
    "        \n",
    "        self.raw_docs = []\n",
    "        \n",
    "        # Contains all the tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.processed_dict = {}\n",
    "        \n",
    "        # Contains all the positive tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.pos_tweets = {}\n",
    "        \n",
    "        # Contains all the neutral tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.neu_tweets = {}\n",
    "            \n",
    "        # Contains all the negativve tweets. Key: Tweet ID, Value: Tweet object\n",
    "        self.neg_tweets = {}\n",
    "        \n",
    "    \n",
    "    def load_corpus(self):\n",
    "        \n",
    "        with open(self.file_path, \"r\", encoding=\"utf8\") as corpus:\n",
    "            data = corpus.readlines()\n",
    "            for line in data:\n",
    "                self.raw_docs.append(line)\n",
    "        \n",
    "        self._corpus_loaded = True\n",
    "    \n",
    "    \n",
    "    def parse_corpus(self):\n",
    "        \n",
    "        for line in self.raw_docs:\n",
    "            _tweet = Tweet(line, self.lemmatizer)\n",
    "            _sentiment = _tweet.sentiment\n",
    "            _id = _tweet.id\n",
    "            \n",
    "            if _sentiment == \"positive\":\n",
    "                self.pos_tweets[_id] = _tweet\n",
    "            elif _sentiment == \"neutral\": \n",
    "                self.neu_tweets[_id] = _tweet\n",
    "            else:\n",
    "                self.neg_tweets[_id] = _tweet\n",
    "            \n",
    "            self.processed_dict[_id] = _tweet\n",
    "                \n",
    "        self._corpus_parsed = True\n",
    "    \n",
    "    \n",
    "    def print_summary_of_corpus(self):\n",
    "        \n",
    "        if not self._corpus_loaded:\n",
    "            self.load_corpus()\n",
    "        \n",
    "        if not self._corpus_parsed:\n",
    "            self.parse_corpus()\n",
    "                      \n",
    "        print(\"Number of training samples: {}.\".format(str(len(self.raw_docs))))\n",
    "        print(\"Number of positive samples: {}.\".format(str(len(self.pos_tweets))))\n",
    "        print(\"Number of neutral samples: {}.\".format(str(len(self.neu_tweets))))\n",
    "        print(\"Number of negative samples: {}.\".format(str(len(self.neg_tweets))))\n",
    "        \n",
    "    \n",
    "    def print_positive_tweets(self):\n",
    "        \n",
    "        for _id in self.pos_tweets:\n",
    "            print(self.pos_tweets[_id])\n",
    "            \n",
    "            \n",
    "    def print_neutral_tweets(self):\n",
    "    \n",
    "        for _id in self.neu_tweets:\n",
    "            print(self.neu_tweets[_id])\n",
    "            \n",
    "            \n",
    "    def print_negative_tweets(self):\n",
    "        \n",
    "        for _id in self.neg_tweets:\n",
    "            print(self.neg_tweets[_id])\n",
    "        \n",
    "\n",
    "c = Corpus(FILE_NAME)\n",
    "c.load_corpus()\n",
    "c.parse_corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a count vector\n",
    "\n",
    "For the second classifier, a count vector is required. Before that can be done, a list of all tweets is needed. So first all, need to go through all the tweets in the corpus and build a new list of just tweet strings. After building the list of tweet strings, use ```CountVectorizer()``` to transform it into a vector by using the ```vectorizer.fit_transform(corpus)``` function. Whilst doing this, we need to also keep track of the labels of each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class CountVectorCorpus:\n",
    "    \n",
    "    def __init__(self, original_corpus):\n",
    "        \n",
    "        self.vectorizer = CountVectorizer()\n",
    "        \n",
    "        self.original_corpus = original_corpus\n",
    "        \n",
    "        self.tweets_list, self.labels_list, self.vector_corpus = self.build_list()\n",
    "        \n",
    "\n",
    "    def build_list(self):\n",
    "        tweets = self.original_corpus.processed_dict\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for _id in tweets:\n",
    "            tweet = tweets[_id]\n",
    "            _sentiment = tweet.sentiment\n",
    "            x_list.append(tweet.tok_str)\n",
    "            y_list.append(_sentiment)\n",
    "            \n",
    "        vect_corp = self.vectorizer.fit_transform(x_list)\n",
    "        return x_list, y_list, vect_corp\n",
    "            \n",
    "\n",
    "cvc = CountVectorCorpus(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "class MultinomialNaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, count_vector):\n",
    "        \n",
    "        # Flag to see if model has been trained\n",
    "        self.trained = False\n",
    "        \n",
    "        self.x_train_tfidf = None\n",
    "        self.count_vector = count_vector\n",
    "        self.tfidf_transformer = None\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        self.tfidf_transformer = TfidfTransformer()\n",
    "        self.x_train_tfidf = self.tfidf_transformer.fit_transform(self.count_vector.vector_corpus)\n",
    "        print(self.x_train_tfidf.shape)\n",
    "        print(self.count_vector.shape)\n",
    "        self.model = MultinomialNB().fit(self.x_train_tfidf, self.count_vector.labels_list)  \n",
    "        self.trained = True\n",
    "        \n",
    "        \n",
    "    def classify_tweet(self, tweet):\n",
    "    \n",
    "        document = tweet.tweet\n",
    "        document = [document]\n",
    "        \n",
    "        if self.trained == False:\n",
    "            self.train()\n",
    "        \n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        \n",
    "        x_new_counts = self.count_vector.vectorizer.transform(document)\n",
    "        x_new_tfidf = self.tfidf_transformer.transform(x_new_counts)\n",
    "        \n",
    "        predicted = self.model.predict(x_new_tfidf)\n",
    "        \n",
    "        for doc, category in zip(document, predicted):\n",
    "            return str(category)\n",
    "        \n",
    "\n",
    "# naiveBayesClassifier = MultinomialNaiveBayesClassifier(cvc)\n",
    "# naiveBayesClassifier.train()\n",
    "# naive_bayes_model = naiveBayesClassifier.model\n",
    "\n",
    "# docs_new = Tweet('071288451742262774\tnegative\tMissed @atmosphere at Soundset due to tornado. Now they are going to in DSM tomorrow. Do i want to put up with crowds and spend $45 more?', wnLemmatizer)\n",
    "\n",
    "# result = naiveBayesClassifier.classify_tweet(docs_new)\n",
    "# print(result)\n",
    "\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, cvc.labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove word embedding\n",
    "\n",
    "Glove word embedding's Twitter dataset is being used. From that, the file called ```glove.twitter.27B.25d.txt``` in particular is being used. \n",
    "\n",
    "The **GloveTwitterWordEmbedding** class is responsible for loading up the file and parsing the dataset. It'll also be responsible for converting each word and sentence into a vector and list of vectors that can then be passed onto the most appropriate machine learning algorithm.\n",
    "\n",
    "After the embeddings have been loaded and parsed, ```get_embeddings_for_sentence(self, sentence)``` can be used for passing on a sentence and getting the embeddings value. In this instance, **sum** function has been used to convert the embeddings for every single word in the sentence into a single vector represening the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "GLOVE_FILE_NAME = \"glove.twitter.27B.25d.txt\"\n",
    "\n",
    "class GloveTwitterWordEmbedding:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        \n",
    "        self.file_name = file_name       \n",
    "        self.file_data = None\n",
    "        \n",
    "        if os.path.isfile(file_name):\n",
    "            print(\"The Glove Word Embedding file has been found.\")\n",
    "        else:\n",
    "            raise IOError(\"File not found: Glove Twitter Embedding file with filename: {}\".format(file_name))\n",
    "        \n",
    "        self.embeddings = { }      \n",
    "        \n",
    "        self.file_loaded = False\n",
    "        self.file_parsed = False\n",
    "        \n",
    "        self.process_file()\n",
    "        \n",
    "    \n",
    "    def load_file(self):\n",
    "        \n",
    "        with open(self.file_name, \"r\", encoding=\"utf8\") as f:\n",
    "            self.file_data = f.readlines()\n",
    "            self.loaded_file = True\n",
    "    \n",
    "    \n",
    "    def process_file(self):\n",
    "        \n",
    "        if self.file_loaded == False:\n",
    "            self.load_file()\n",
    "        \n",
    "        for i in range(len(self.file_data)):\n",
    "            _emb = self.file_data[i]\n",
    "            _emb_parts = _emb.split()\n",
    "            \n",
    "            # Storing the word embeddings in a dictionary where the key is the word and the value is \n",
    "            # a numpy array of the word embedding vector.\n",
    "            self.embeddings[str(_emb_parts[0])] = np.array(_emb_parts[1:], dtype=np.float32)  \n",
    "        \n",
    "        self.file_parsed = True\n",
    "            \n",
    "    \n",
    "    def get_embeddings_for_sentence(self, sentence):\n",
    "        \n",
    "        _embeddings = []\n",
    "        \n",
    "        _words = sentence.split()\n",
    "        for i in range(len(_words)):\n",
    "            \n",
    "            _embedding = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=np.float32)\n",
    "            if str(_words[i]) in self.embeddings:\n",
    "                _embedding = self.embeddings[str(_words[i])]\n",
    "            \n",
    "            _embeddings.append(_embedding)\n",
    "        \n",
    "        _embeddings = np.array(_embeddings, dtype=np.float32)\n",
    "        _emb_sum = np.sum(_embeddings, axis=0, dtype=np.float32)\n",
    "        \n",
    "        return _emb_sum\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding, SVM classifier\n",
    "\n",
    "Using SVM with word embedding as the third (final) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "class SVMClassifier:\n",
    "    \n",
    "    def __init__(self, original_corpus, word_embedding):\n",
    "        \n",
    "        self.original_corpus = original_corpus\n",
    "        self.word_embedding = word_embedding\n",
    "        \n",
    "        self.target_dict = {\n",
    "                \"negative\": 1, \n",
    "                \"neutral\": 2,\n",
    "                \"positive\": 3\n",
    "            }\n",
    "        \n",
    "        self.reverse_target_dict = {\n",
    "            1.: \"negative\",\n",
    "            2.: \"neutral\",\n",
    "            3.: \"positive\"\n",
    "        }\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        self.prepare_data()\n",
    "        \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        _tweets = self.original_corpus.processed_dict\n",
    "        \n",
    "        for _id in _tweets:\n",
    "            \n",
    "            _tweet = _tweets[_id]\n",
    "            _sentiment = _tweet.sentiment\n",
    "            _tweet_str = _tweet.tweet\n",
    "            \n",
    "            \n",
    "            \n",
    "            _tweet_embedding_vector = self.word_embedding.get_embeddings_for_sentence(_tweet_str)\n",
    "            \n",
    "            self.train_x.append(_tweet_embedding_vector)\n",
    "            self.train_y.append(self.target_dict[_sentiment] if _sentiment in self.target_dict else 0)\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        import pickle\n",
    "\n",
    "        my_svm_model_name = \"my_svm_model.pkl\"\n",
    "\n",
    "        if os.path.isfile(my_svm_model_name):\n",
    "            # Files exists so read it\n",
    "            print(\"Reading pickle\")\n",
    "            with open(my_svm_model_name, 'rb') as f:\n",
    "                self.model = pickle.load(f)\n",
    "        else:\n",
    "            # Create it and save it\n",
    "            \n",
    "            self.model = OneVsRestClassifier(estimator=SVC(gamma='auto', random_state=0))\n",
    "            self.model.fit(self.train_x, self.train_y)\n",
    "            \n",
    "            print(\"writing pickle\")\n",
    "            with open(my_svm_model_name, 'wb') as f:\n",
    "                pickle.dump(self.model, f)\n",
    "        \n",
    "    \n",
    "    def classify_tweet(self, tweet):\n",
    "        \n",
    "        _tweet_str = tweet.tweet\n",
    "        _embedding = self.word_embedding.get_embeddings_for_sentence(_tweet_str)\n",
    "        \n",
    "        prediction = self.model.predict(_embedding)\n",
    "        \n",
    "        return self.reverse_target_dict[prediction[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon classifier class\n",
    "\n",
    "This classifier uses the word's positive and negative ratings to work out the overall sentiment. The method ```classify_tweet(Tweet)``` takes the Tweet class as an argument. It extracts the Tweet's text and then classifies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexiconClassifier: \n",
    "    \n",
    "    def __init__(self, word_sentiment):\n",
    "        \n",
    "        self.min_neut = -0.15\n",
    "        self.max_neut = 0.15\n",
    "        \n",
    "        self.word_sentiment = word_sentiment\n",
    "        \n",
    "        \n",
    "    def classify_tweet(self, tweet):\n",
    "        \n",
    "        words_dict = self.word_sentiment.all_words   \n",
    "        score = float(0)\n",
    "        tokens = tweet.tokens\n",
    "    \n",
    "        sentiment = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            if tok in words_dict:\n",
    "                score += words_dict[tok]\n",
    "        \n",
    "        if score > self.min_neut and score < self.max_neut:\n",
    "            sentiment = \"neutral\"\n",
    "        elif score > self.max_neut:\n",
    "            sentiment = \"positive\"\n",
    "        else:\n",
    "            sentiment = \"negative\"\n",
    "            \n",
    "        return str(sentiment)\n",
    "\n",
    "\n",
    "# Declaring the lexicon Classifier\n",
    "lc = LexiconClassifier(ws)\n",
    "# print(lc.classify_tweet(Tweet(\"382489758445350006\tnegative\thigh prozac\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loader class\n",
    "\n",
    "Class for loading the test sets. It accepts the **file_name** and the **classifier** as arguments. It returns the dictionary of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestData:\n",
    "    \n",
    "    def __init__(self, file_name, corpus, classifier):\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        self.corpus = corpus\n",
    "        self.classifier = classifier   \n",
    "        \n",
    "        self.classified_dict = {}\n",
    "\n",
    "    def run_classifier(self):\n",
    "        \n",
    "        # Predictions dictionary\n",
    "        results_dict = {}\n",
    "        \n",
    "        # Tweets dict\n",
    "        td = self.corpus.processed_dict\n",
    "        \n",
    "        for _id in td:\n",
    "            _tweet = td[_id]\n",
    "            classification = self.classifier.classify_tweet(_tweet)\n",
    "            results_dict[str(_id)] = classification\n",
    "        \n",
    "        return results_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main classification\n",
    "\n",
    "The code below is part of the skeleton code that was provided in ```classification.py``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Glove Word Embedding file has been found.\n",
      "Training is NOT required for: lex_classifier\n",
      "The file has been found.\n",
      "twitter-test1.txt (lex_classifier): 0.445\n",
      "            positive  negative  neutral\n",
      "positive    0.501     0.108     0.391     \n",
      "negative    0.293     0.261     0.447     \n",
      "neutral     0.373     0.140     0.487     \n",
      "\n",
      "The file has been found.\n",
      "twitter-test2.txt (lex_classifier): 0.436\n",
      "            positive  negative  neutral\n",
      "positive    0.614     0.077     0.309     \n",
      "negative    0.363     0.181     0.456     \n",
      "neutral     0.528     0.098     0.374     \n",
      "\n",
      "The file has been found.\n",
      "twitter-test3.txt (lex_classifier): 0.397\n",
      "            positive  negative  neutral\n",
      "positive    0.491     0.129     0.380     \n",
      "negative    0.330     0.213     0.457     \n",
      "neutral     0.460     0.118     0.422     \n",
      "\n",
      "The Glove Word Embedding file has been found.\n",
      "Training naive_bayes\n",
      "(45101, 41312)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorCorpus' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e80ba5ddb8e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'naive_bayes'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mnaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svm'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e40c99f9e376>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorCorpus' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for classifier in ['lex_classifier', 'naive_bayes', 'svm']: # You may rename the names of the classifiers to something more descriptive\n",
    "    \n",
    "    # Used for Multinomial Naive Bayes classifier\n",
    "    count_vector = CountVectorCorpus(c)\n",
    "    naiveBayesClassifier = MultinomialNaiveBayesClassifier(count_vector)\n",
    "    \n",
    "    # Used for word_embeddings and SVM classifier      \n",
    "    word_embedding = GloveTwitterWordEmbedding(GLOVE_FILE_NAME)\n",
    "    my_svm = SVMClassifier(c, word_embedding)\n",
    "    \n",
    "    \n",
    "    if classifier == 'lex_classifier':\n",
    "        print('Training is NOT required for: ' + classifier)\n",
    "\n",
    "    elif classifier == 'naive_bayes':\n",
    "        print('Training ' + classifier)\n",
    "        naiveBayesClassifier.train()\n",
    "        \n",
    "    elif classifier == 'svm':\n",
    "        print('Training ' + classifier)\n",
    "        my_svm.train()\n",
    "        \n",
    "            \n",
    "    for testset in testsets.testsets:\n",
    "        # TODO: classify tweets in test set\n",
    "        predictions = {}\n",
    "    \n",
    "        if classifier == 'lex_classifier':\n",
    "            test_corpus = Corpus(testset)\n",
    "            test_corpus.load_corpus()\n",
    "            test_corpus.parse_corpus()\n",
    "\n",
    "            td1 = TestData(testset, test_corpus, lc)\n",
    "            predictions = td1.run_classifier()\n",
    "            \n",
    "        elif classifier == 'naive_bayes':\n",
    "            test_corpus = Corpus(testset)\n",
    "            test_corpus.load_corpus()\n",
    "            test_corpus.parse_corpus()\n",
    "            \n",
    "            td2 = TestData(testset, test_corpus, naiveBayesClassifier)\n",
    "            predictions = td2.run_classifier()\n",
    "\n",
    "        elif classifier == 'svm':\n",
    "            \n",
    "            test_corpus = Corpus(testset)\n",
    "            test_corpus.load_corpus()\n",
    "            test_corpus.parse_corpus()\n",
    "            \n",
    "            td3 = TestData(testset, test_corpus, my_svm)\n",
    "            predictions = td3.run_classifier()\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        evaluation.evaluate(predictions, testset, classifier)\n",
    "        evaluation.confusion(predictions, testset, classifier)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
